{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import feather\n",
    "import datetime\n",
    "# import talib\n",
    "# import mysql.connector\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import seed\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix,roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from py5paisa import FivePaisaClient"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:27.865194Z",
     "start_time": "2019-10-02T17:27:23.864966Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "seed(1)\n",
    "set_random_seed(2)\n",
    "client = FivePaisaClient(email=\"52119099\", passwd=\"#bhola@1996\", dob=\"19840101\")\n",
    "client.login()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 12:01:53 | Logged in!!\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:27.882195Z",
     "start_time": "2019-10-02T17:27:27.867195Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "scriptDf = pd.read_csv('../HistoricalData/NSECashScripts.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "symbol = 'banknifty'\n",
    "end = datetime.datetime.now()\n",
    "start = end - datetime.timedelta(100)\n",
    "\n",
    "scode = int(scriptDf[scriptDf['Name']==symbol.upper()]['Scripcode'])\n",
    "data = client.historical_data('N' , 'C' , scode ,'1m', start ,end)\n",
    "data.set_index('Datetime' , inplace=True)\n",
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                         Open      High       Low     Close  Volume\n",
       "Datetime                                                           \n",
       "2021-05-10T11:53:00  33203.80  33206.50  33199.75  33201.60       0\n",
       "2021-05-10T11:54:00  33203.35  33214.25  33198.50  33204.85       0\n",
       "2021-05-10T11:55:00  33205.15  33208.60  33201.90  33205.15       0\n",
       "2021-05-10T11:56:00  33206.00  33206.00  33185.00  33189.10       0\n",
       "2021-05-10T11:57:00  33188.40  33193.95  33177.00  33179.00       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-05-10T11:53:00</th>\n",
       "      <td>33203.80</td>\n",
       "      <td>33206.50</td>\n",
       "      <td>33199.75</td>\n",
       "      <td>33201.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-10T11:54:00</th>\n",
       "      <td>33203.35</td>\n",
       "      <td>33214.25</td>\n",
       "      <td>33198.50</td>\n",
       "      <td>33204.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-10T11:55:00</th>\n",
       "      <td>33205.15</td>\n",
       "      <td>33208.60</td>\n",
       "      <td>33201.90</td>\n",
       "      <td>33205.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-10T11:56:00</th>\n",
       "      <td>33206.00</td>\n",
       "      <td>33206.00</td>\n",
       "      <td>33185.00</td>\n",
       "      <td>33189.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-10T11:57:00</th>\n",
       "      <td>33188.40</td>\n",
       "      <td>33193.95</td>\n",
       "      <td>33177.00</td>\n",
       "      <td>33179.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "def getData(symbol,start,end):   \n",
    "    try:\n",
    "        scode = int(scriptDf[scriptDf['Name']==symbol.upper()]['Scripcode'])\n",
    "        data = client.historical_data('N' , 'C' , scode ,'1m', start ,end)\n",
    "        data.set_index('Datetime' , inplace=True)\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "            print(\"Error : \" , e) "
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:28.049205Z",
     "start_time": "2019-10-02T17:27:28.044205Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "def balanceData(X,y):\n",
    "    sm = SMOTE(random_state=2)\n",
    "    X, y = sm.fit_resample(X, y)    \n",
    "    return pd.DataFrame(X),pd.Series(y)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:29.486287Z",
     "start_time": "2019-10-02T17:27:29.481287Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "end = datetime.datetime.today()\n",
    "start = end - datetime.timedelta(100)\n",
    "symbol = 'BANKNIFTY'\n",
    "#Text formatting\n",
    "boldTextStart = \"\\033[1m\"\n",
    "boldTextEnd = \"\\033[0;0m\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:31.298391Z",
     "start_time": "2019-10-02T17:27:31.291390Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "dataset = getData(symbol,start,end)\n",
    "data = dataset.copy()\n",
    "data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(26326, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:34.833593Z",
     "start_time": "2019-10-02T17:27:31.647411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "data['Close'].plot(figsize=(15,7))\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"415.23744pt\" version=\"1.1\" viewBox=\"0 0 890.2125 415.23744\" width=\"890.2125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-18T12:05:06.871739</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 415.23744 \nL 890.2125 415.23744 \nL 890.2125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 46.0125 349.92 \nL 883.0125 349.92 \nL 883.0125 7.2 \nL 46.0125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 118.333234 349.92 \nL 118.333234 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m262f19c04a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"118.333234\" xlink:href=\"#m262f19c04a\" y=\"349.92\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 2021-05-15 -->\n      <g transform=\"translate(66.963783 392.558252)rotate(-30)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"290.576172\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"354.199219\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"417.822266\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"453.90625\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"517.529297\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 247.677899 349.92 \nL 247.677899 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"247.677899\" xlink:href=\"#m262f19c04a\" y=\"349.92\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2021-06-01 -->\n      <g transform=\"translate(196.308448 392.558252)rotate(-30)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"290.576172\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"354.199219\" xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"417.822266\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"453.90625\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"517.529297\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 354.197035 349.92 \nL 354.197035 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"354.197035\" xlink:href=\"#m262f19c04a\" y=\"349.92\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2021-06-15 -->\n      <g transform=\"translate(302.827583 392.558252)rotate(-30)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"290.576172\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"354.199219\" xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"417.822266\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"453.90625\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"517.529297\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 475.93319 349.92 \nL 475.93319 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"475.93319\" xlink:href=\"#m262f19c04a\" y=\"349.92\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2021-07-01 -->\n      <g transform=\"translate(424.563739 392.558252)rotate(-30)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"290.576172\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"354.199219\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"417.822266\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"453.90625\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"517.529297\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 582.452326 349.92 \nL 582.452326 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"582.452326\" xlink:href=\"#m262f19c04a\" y=\"349.92\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2021-07-15 -->\n      <g transform=\"translate(531.082875 392.558252)rotate(-30)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"290.576172\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"354.199219\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"417.822266\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"453.90625\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"517.529297\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 711.796991 349.92 \nL 711.796991 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"711.796991\" xlink:href=\"#m262f19c04a\" y=\"349.92\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 2021-08-01 -->\n      <g transform=\"translate(660.42754 392.558252)rotate(-30)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"290.576172\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"354.199219\" xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"417.822266\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"453.90625\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"517.529297\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 818.316127 349.92 \nL 818.316127 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"818.316127\" xlink:href=\"#m262f19c04a\" y=\"349.92\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 2021-08-15 -->\n      <g transform=\"translate(766.946675 392.558252)rotate(-30)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"290.576172\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"354.199219\" xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"417.822266\" xlink:href=\"#DejaVuSans-2d\"/>\n       <use x=\"453.90625\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"517.529297\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- Datetime -->\n     <g transform=\"translate(441.265625 405.957752)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1259 4147 \nL 1259 519 \nL 2022 519 \nQ 2988 519 3436 956 \nQ 3884 1394 3884 2338 \nQ 3884 3275 3436 3711 \nQ 2988 4147 2022 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 1925 4666 \nQ 3281 4666 3915 4102 \nQ 4550 3538 4550 2338 \nQ 4550 1131 3912 565 \nQ 3275 0 1925 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-44\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-44\"/>\n      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"138.28125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"177.490234\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"239.013672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"278.222656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"306.005859\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"403.417969\" xlink:href=\"#DejaVuSans-65\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 46.0125 345.075061 \nL 883.0125 345.075061 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"md3b682379b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md3b682379b\" y=\"345.075061\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 32000 -->\n      <g transform=\"translate(7.2 348.874279)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 46.0125 270.174554 \nL 883.0125 270.174554 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md3b682379b\" y=\"270.174554\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 33000 -->\n      <g transform=\"translate(7.2 273.973773)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 46.0125 195.274048 \nL 883.0125 195.274048 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md3b682379b\" y=\"195.274048\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 34000 -->\n      <g transform=\"translate(7.2 199.073267)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 46.0125 120.373542 \nL 883.0125 120.373542 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md3b682379b\" y=\"120.373542\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 35000 -->\n      <g transform=\"translate(7.2 124.17276)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 46.0125 45.473035 \nL 883.0125 45.473035 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#md3b682379b\" y=\"45.473035\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 36000 -->\n      <g transform=\"translate(7.2 49.272254)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_25\">\n    <path clip-path=\"url(#p51cb4b8e3e)\" d=\"M 84.057955 255.074612 \nL 84.110791 254.67764 \nL 84.079089 256.767364 \nL 84.142494 255.876048 \nL 84.211181 258.662347 \nL 84.232316 255.209433 \nL 84.248167 255.374214 \nL 84.322139 251.554288 \nL 84.264018 256.471507 \nL 84.374976 253.797559 \nL 84.454231 251.419468 \nL 84.385543 254.651424 \nL 84.464798 252.595406 \nL 84.491217 262.257571 \nL 84.575756 258.276609 \nL 84.596891 256.692463 \nL 84.660295 259.827049 \nL 84.68143 258.860833 \nL 84.718415 256.149435 \nL 84.702564 259.014379 \nL 84.792387 257.475174 \nL 84.866359 262.422352 \nL 84.818806 254.909831 \nL 84.908628 260.819481 \nL 84.950898 258.520036 \nL 84.993167 260.965537 \nL 85.051288 262.770639 \nL 85.098841 259.493742 \nL 85.125259 261.186494 \nL 85.188664 255.625131 \nL 85.193947 255.108318 \nL 85.199231 255.831107 \nL 85.204515 255.374214 \nL 90.831642 293.011719 \nL 90.842209 292.566061 \nL 90.916181 285.135931 \nL 90.974301 286.873622 \nL 91.074691 281.34971 \nL 90.990152 288.483983 \nL 91.10111 282.667959 \nL 91.196216 279.357356 \nL 91.217351 284.821348 \nL 91.264904 281.701742 \nL 91.323025 284.551707 \nL 91.328308 285.911151 \nL 91.407564 278.005402 \nL 91.560791 260.224022 \nL 91.4604 279.930345 \nL 91.581925 263.452234 \nL 91.592493 261.519801 \nL 91.640046 268.713994 \nL 91.655897 267.103634 \nL 91.682315 269.305708 \nL 91.714017 266.096222 \nL 91.740436 266.875187 \nL 91.84611 264.163789 \nL 91.80384 267.343315 \nL 91.856677 264.867853 \nL 92.041606 278.896718 \nL 92.057457 278.765642 \nL 92.073308 277.496079 \nL 92.131429 284.488041 \nL 92.136712 283.274653 \nL 92.152563 288.536414 \nL 92.242386 282.645489 \nL 92.258237 286.109637 \nL 92.316358 280.585725 \nL 92.337493 282.596803 \nL 92.353344 279.286201 \nL 92.411464 284.825093 \nL 92.44845 281.780388 \nL 92.496003 286.229478 \nL 92.517138 281.581901 \nL 92.617528 278.099028 \nL 92.575258 282.237281 \nL 92.628095 281.035128 \nL 92.643946 281.529471 \nL 92.670365 279.724369 \nL 92.680932 277.533529 \nL 92.760188 280.941502 \nL 92.776039 280.6419 \nL 92.813024 282.78031 \nL 98.440151 291.532434 \nL 98.56696 302.040975 \nL 98.450719 291.034345 \nL 98.582811 299.352047 \nL 98.688485 295.393555 \nL 98.614513 300.763921 \nL 98.704336 296.16503 \nL 98.709619 294.962877 \nL 98.783591 302.415477 \nL 98.804726 305.714845 \nL 98.878697 301.763843 \nL 98.899832 305.119386 \nL 98.931534 300.456829 \nL 99.021357 302.209501 \nL 99.031924 301.883684 \nL 99.037208 302.509103 \nL 99.090045 314.534379 \nL 99.153449 309.433655 \nL 99.31196 300.374439 \nL 99.164017 309.568476 \nL 99.317243 301.658982 \nL 99.327811 303.583925 \nL 99.385931 295.764312 \nL 99.41235 297.790371 \nL 99.465187 292.487415 \nL 99.528591 296.595708 \nL 99.61313 301.112209 \nL 99.650116 298.786548 \nL 99.71352 295.842958 \nL 99.734655 299.689099 \nL 99.755789 298.104953 \nL 99.85618 305.707355 \nL 99.887882 302.999701 \nL 99.967137 295.258734 \nL 100.046392 297.337223 \nL 100.104513 296.060169 \nL 100.199619 304.351655 \nL 100.204903 303.553965 \nL 100.247172 312.924018 \nL 100.25774 315.721552 \nL 100.284158 307.497477 \nL 100.347562 310.785609 \nL 100.389832 309.830627 \nL 100.368697 311.148876 \nL 100.421534 310.781864 \nL 113.657171 308.444968 \nL 113.683589 299.258421 \nL 113.746993 320.736141 \nL 113.757561 319.87104 \nL 113.863235 304.044563 \nL 113.94249 305.79349 \nL 113.974192 312.620671 \nL 113.995327 304.928389 \nL 114.069298 308.568554 \nL 114.074582 306.965683 \nL 114.14327 313.571908 \nL 114.164405 312.482105 \nL 114.24366 319.331757 \nL 114.28593 319.01343 \nL 114.354617 315.744022 \nL 114.322915 319.829845 \nL 114.391603 319.054625 \nL 114.550114 334.113372 \nL 114.555398 331.061176 \nL 114.64522 326.203878 \nL 114.576532 333.967316 \nL 114.682206 327.728103 \nL 114.708625 330.458227 \nL 114.761461 323.511205 \nL 114.782596 325.66834 \nL 114.851284 328.948982 \nL 114.909405 323.016862 \nL 114.935823 327.537107 \nL 115.015078 323.52244 \nL 115.025646 321.395266 \nL 115.078483 331.207232 \nL 115.104901 326.140213 \nL 115.157738 333.469227 \nL 115.23171 331.840141 \nL 115.236993 331.900062 \nL 115.242277 331.59297 \nL 115.358518 325.679575 \nL 115.295114 334.034726 \nL 115.374369 326.514715 \nL 115.49061 331.948747 \nL 115.411355 322.649849 \nL 115.495894 330.993766 \nL 115.501178 329.844043 \nL 115.543447 334.341818 \nL 115.601568 331.169782 \nL 115.627986 333.386837 \nL 115.638554 332.555441 \nL 136.4827 316.335736 \nL 136.487984 317.324423 \nL 136.509118 305.834685 \nL 136.530253 306.47883 \nL 136.64121 288.510198 \nL 136.694047 291.468768 \nL 136.709898 293.895545 \nL 136.757452 285.004855 \nL 136.820856 276.511137 \nL 136.873693 278.851778 \nL 136.889544 281.237359 \nL 136.931813 273.19679 \nL 136.968799 276.076714 \nL 137.08504 271.470333 \nL 137.032203 278.518471 \nL 137.090324 272.103242 \nL 137.111459 273.945795 \nL 137.159012 268.976146 \nL 137.301671 262.332471 \nL 137.206565 269.949853 \nL 137.312239 263.808011 \nL 137.454898 260.283943 \nL 137.32809 264.785463 \nL 137.460182 261.261394 \nL 137.465466 263.57582 \nL 137.502452 260.403783 \nL 137.571139 262.605858 \nL 137.581707 263.572075 \nL 137.592274 261.542271 \nL 137.703232 255.621386 \nL 137.708515 256.449037 \nL 137.819473 265.777895 \nL 137.724366 256.097004 \nL 137.867026 262.587133 \nL 137.993834 255.965928 \nL 138.136494 249.086317 \nL 138.162912 249.333488 \nL 138.168196 250.32592 \nL 138.221033 241.330369 \nL 138.236884 243.914437 \nL 138.33199 234.297212 \nL 138.384827 238.465425 \nL 138.442948 232.990198 \nL 138.464083 234.656734 \nL 144.09121 197.228951 \nL 144.159898 185.619373 \nL 144.196883 199.408556 \nL 144.202167 196.191579 \nL 144.260288 191.341771 \nL 144.318408 199.176364 \nL 144.387096 213.104114 \nL 144.334259 198.506005 \nL 144.455784 207.44538 \nL 144.561458 198.344969 \nL 144.598444 199.453496 \nL 144.603727 199.614532 \nL 144.619578 197.873096 \nL 144.682983 187.42822 \nL 144.741103 191.236911 \nL 144.746387 190.626472 \nL 144.788656 198.315009 \nL 144.79394 198.359949 \nL 144.820358 195.989348 \nL 144.846777 201.561946 \nL 144.904897 196.914369 \nL 145.010571 193.135639 \nL 144.931316 198.824332 \nL 145.03699 195.918192 \nL 145.116245 199.899154 \nL 145.047557 194.599943 \nL 145.169082 198.277558 \nL 145.248337 194.82839 \nL 145.264188 198.344969 \nL 145.274756 198.243853 \nL 145.348727 200.273657 \nL 145.29589 196.626002 \nL 145.354011 197.959231 \nL 145.459685 192.060816 \nL 145.464968 192.424084 \nL 145.581209 199.161384 \nL 145.512521 189.869976 \nL 145.586493 199.116444 \nL 145.59706 199.790549 \nL 145.697451 196.412536 \nL 145.755571 206.572789 \nL 145.818975 203.370793 \nL 145.834826 197.910546 \nL 145.892947 205.115975 \nL 145.929933 202.28848 \nL 145.935216 202.28099 \nL 145.961635 204.351989 \nL 146.030323 198.704491 \nL 146.046174 197.569749 \nL 146.062025 199.427281 \nL 146.072592 198.618356 \nL 151.699719 212.886902 \nL 151.705003 217.032645 \nL 151.794826 200.925291 \nL 151.81596 199.580827 \nL 151.842379 206.954782 \nL 151.879365 204.719002 \nL 151.905783 201.247363 \nL 151.990322 207.778688 \nL 151.995606 206.198287 \nL 152.069577 215.924118 \nL 152.074861 215.133917 \nL 152.080145 214.796865 \nL 152.106563 218.223563 \nL 152.117131 221.616556 \nL 152.206953 213.972959 \nL 152.249223 218.077507 \nL 152.317911 214.811845 \nL 152.328478 214.04037 \nL 152.381315 220.433128 \nL 152.391882 219.122369 \nL 152.40245 219.657908 \nL 152.476421 217.800375 \nL 152.645499 206.887372 \nL 152.650783 206.999722 \nL 152.740606 213.796943 \nL 152.772308 211.725944 \nL 152.825145 209.962037 \nL 152.793443 212.137897 \nL 152.840996 211.883235 \nL 152.909684 215.452244 \nL 152.957237 214.250091 \nL 152.988939 212.725866 \nL 153.025925 214.223876 \nL 153.136882 221.957353 \nL 153.152733 220.627869 \nL 153.221421 215.56834 \nL 153.258407 219.938785 \nL 153.337662 218.717906 \nL 153.374648 223.417913 \nL 153.490889 209.984507 \nL 153.54901 216.826669 \nL 153.622981 221.961098 \nL 153.575428 216.575752 \nL 153.675818 220.803886 \nL 153.681102 218.852727 \nL 159.308229 211.624829 \nL 159.334647 208.460282 \nL 159.329364 212.695906 \nL 159.361066 211.81957 \nL 159.429754 222.848669 \nL 159.477307 219.579262 \nL 159.482591 219.691613 \nL 159.498442 218.42954 \nL 159.551279 220.013685 \nL 159.62525 212.441244 \nL 159.72564 220.197191 \nL 159.746775 218.234798 \nL 159.836598 219.830179 \nL 159.76791 217.216151 \nL 159.847165 217.991372 \nL 159.863016 216.34356 \nL 159.942271 219.054959 \nL 159.947555 218.598066 \nL 159.96869 221.024842 \nL 160.058513 218.729142 \nL 160.100782 219.717828 \nL 160.137768 216.807944 \nL 160.16947 218.991293 \nL 160.185321 220.429383 \nL 160.21174 217.276072 \nL 160.248725 218.579341 \nL 160.254009 218.103722 \nL 160.285711 222.942295 \nL 160.31213 222.668908 \nL 160.338548 224.882218 \nL 160.375534 221.556636 \nL 160.41252 222.642693 \nL 160.460073 220.875041 \nL 160.502342 223.470344 \nL 160.528761 221.927393 \nL 160.666137 229.252663 \nL 160.539328 221.811297 \nL 160.708406 227.646047 \nL 160.824647 222.728829 \nL 160.718973 228.499913 \nL 160.845782 224.728672 \nL 161.020144 236.967415 \nL 161.030711 234.473228 \nL 161.051846 233.495776 \nL 161.067697 237.971082 \nL 161.078264 237.004865 \nL 161.136385 248.655639 \nL 161.226207 247.35237 \nL 161.289612 240.131961 \nL 166.916739 217.849061 \nL 166.922022 218.339659 \nL 166.974859 213.579732 \nL 167.106951 195.824567 \nL 167.112235 199.711903 \nL 167.170356 194.131815 \nL 167.159788 201.135013 \nL 167.217909 199.311185 \nL 167.223193 200.404733 \nL 167.254895 192.498984 \nL 167.302448 193.925839 \nL 167.360568 187.263439 \nL 167.429256 190.514121 \nL 167.471526 191.82488 \nL 167.439824 189.58161 \nL 167.53493 191.11707 \nL 167.640604 184.173793 \nL 167.682873 185.780409 \nL 167.825533 191.293086 \nL 167.862519 189.585355 \nL 167.920639 186.671725 \nL 167.973476 188.16599 \nL 168.147838 196.270225 \nL 168.285214 188.061129 \nL 168.184824 198.876762 \nL 168.343334 188.390691 \nL 168.390887 197.547278 \nL 168.427873 186.832761 \nL 168.454292 188.694039 \nL 168.470143 191.776194 \nL 168.517696 176.665017 \nL 168.52298 178.084382 \nL 168.665639 156.689052 \nL 168.670923 156.737737 \nL 168.68149 153.322274 \nL 168.755462 158.269453 \nL 168.787164 153.427135 \nL 168.898121 144.431584 \nL 189.742268 120.822945 \nL 189.826807 139.038748 \nL 189.77397 115.471303 \nL 189.853225 126.459208 \nL 189.869076 124.313308 \nL 189.87436 125.654027 \nL 190.001168 153.464585 \nL 190.011736 145.828479 \nL 190.096275 128.94216 \nL 190.149112 130.574991 \nL 190.212516 136.548306 \nL 190.170246 129.76981 \nL 190.244218 130.616186 \nL 190.291771 123.89761 \nL 190.365743 127.024707 \nL 190.471417 132.260252 \nL 190.386878 125.193389 \nL 190.481984 130.391484 \nL 190.513686 130.556265 \nL 190.503119 128.612597 \nL 190.534821 129.279212 \nL 190.540105 128.350446 \nL 190.603509 135.7244 \nL 190.624644 131.896985 \nL 190.725034 135.057786 \nL 190.661629 129.590049 \nL 190.735601 131.99061 \nL 190.746168 131.743438 \nL 190.888828 145.749833 \nL 190.894112 143.544013 \nL 190.983934 128.930924 \nL 191.036771 134.132765 \nL 191.110743 129.432758 \nL 191.142445 134.990375 \nL 191.168863 132.440013 \nL 191.184714 131.739693 \nL 191.205849 135.413563 \nL 191.211133 137.997631 \nL 191.242835 131.076824 \nL 191.306239 132.383838 \nL 191.36436 136.4397 \nL 191.316807 132.136666 \nL 191.433048 134.320016 \nL 191.52287 104.880372 \nL 191.56514 109.438068 \nL 191.707799 135.173882 \nL 191.718367 132.237782 \nL 191.723651 124.59793 \nL 197.350778 119.808043 \nL 197.356061 118.276327 \nL 197.424749 129.372837 \nL 197.430033 128.541442 \nL 197.54099 141.398114 \nL 197.467019 127.623911 \nL 197.577976 139.480661 \nL 197.599111 135.585834 \nL 197.678366 138.900182 \nL 197.68365 144.424094 \nL 197.757621 137.81787 \nL 197.789324 140.443132 \nL 197.847444 147.214138 \nL 197.905565 143.521543 \nL 197.990104 153.078848 \nL 198.037657 148.1841 \nL 198.148614 139.39078 \nL 198.164465 141.356918 \nL 198.270139 151.13518 \nL 198.28599 148.828244 \nL 198.407515 143.296842 \nL 198.465636 145.169354 \nL 198.534323 149.109121 \nL 198.566026 143.723774 \nL 198.576593 145.506407 \nL 198.745671 150.734462 \nL 198.603011 144.191903 \nL 198.750955 150.292549 \nL 198.756238 148.611033 \nL 198.809075 162.325315 \nL 198.835494 156.415665 \nL 198.925316 162.939499 \nL 198.957018 162.606192 \nL 199.067976 144.019631 \nL 198.983437 163.261572 \nL 199.115529 146.105611 \nL 199.136664 155.247217 \nL 199.226486 150.648326 \nL 199.300458 139.795243 \nL 199.33216 145.689913 \nL 204.959287 140.536758 \nL 205.091379 159.610172 \nL 205.096663 159.535271 \nL 205.239323 146.240431 \nL 205.24989 149.311352 \nL 205.265741 148.154139 \nL 205.276308 150.659561 \nL 205.313294 149.191511 \nL 205.418968 134.072844 \nL 205.434819 136.713087 \nL 205.455954 138.574365 \nL 205.514074 130.586226 \nL 205.524642 132.844476 \nL 205.588046 130.833397 \nL 205.55106 134.937945 \nL 205.625032 134.305036 \nL 205.79411 141.828792 \nL 205.804677 141.319468 \nL 205.873365 133.597226 \nL 205.931486 135.806791 \nL 205.942053 136.567031 \nL 205.989606 131.028139 \nL 206.005457 131.428856 \nL 206.021308 129.515148 \nL 206.05301 133.848143 \nL 206.068862 133.024237 \nL 206.09528 136.353565 \nL 206.132266 131.979375 \nL 206.185103 135.226312 \nL 206.311911 129.238017 \nL 206.317195 130.024472 \nL 206.375315 140.491818 \nL 206.359464 129.163116 \nL 206.475705 138.784086 \nL 206.565528 139.825203 \nL 206.602514 134.578423 \nL 206.623649 138.428309 \nL 206.718755 137.338506 \nL 206.724039 136.615716 \nL 206.776876 139.772773 \nL 206.79801 146.094376 \nL 206.893117 145.832224 \nL 206.903684 144.236843 \nL 206.94067 144.697481 \nL 212.567797 143.495328 \nL 212.710456 154.588093 \nL 212.726308 154.535663 \nL 212.837265 145.618757 \nL 212.942939 147.996848 \nL 213.022194 155.052476 \nL 213.080315 151.284981 \nL 213.201839 147.862027 \nL 213.21769 148.618523 \nL 213.228258 149.397488 \nL 213.254676 146.071905 \nL 213.25996 146.356527 \nL 213.381485 135.361133 \nL 213.471307 130.986943 \nL 213.439605 136.274919 \nL 213.50301 131.316506 \nL 213.576981 137.469582 \nL 213.629818 136.012767 \nL 213.650953 138.244802 \nL 213.70379 135.66448 \nL 213.825314 125.38064 \nL 213.867584 127.174508 \nL 213.930988 120.463422 \nL 214.010244 124.466854 \nL 214.020811 125.448051 \nL 214.084215 120.778004 \nL 214.100066 121.100077 \nL 214.10535 120.998961 \nL 214.110634 121.792906 \nL 214.115917 121.249878 \nL 214.242726 136.743047 \nL 214.253293 139.042493 \nL 214.30613 129.354112 \nL 214.427655 116.984294 \nL 214.369534 130.473875 \nL 214.432939 117.823179 \nL 214.501626 106.591848 \nL 214.54918 110.041017 \nL 220.176307 89.342262 \nL 220.234427 101.034231 \nL 220.260846 88.735568 \nL 220.287264 92.233421 \nL 220.435207 105.056388 \nL 220.451058 103.625788 \nL 220.572583 96.693747 \nL 220.509179 108.453126 \nL 220.583151 98.899566 \nL 220.672973 104.262443 \nL 220.609569 98.532554 \nL 220.709959 102.3375 \nL 220.810349 113.291699 \nL 220.910739 109.838785 \nL 220.931874 105.247384 \nL 220.984711 113.490185 \nL 221.021697 109.659024 \nL 221.02698 111.119584 \nL 221.122087 107.48691 \nL 221.12737 106.659259 \nL 221.206626 110.52787 \nL 221.222477 116.976804 \nL 221.307016 110.366834 \nL 221.317583 111.340541 \nL 221.344002 114.126839 \nL 221.375704 108.074878 \nL 221.417973 109.366912 \nL 221.428541 110.041017 \nL 221.439108 108.730258 \nL 221.555349 103.06029 \nL 221.465526 113.673691 \nL 221.560633 103.947861 \nL 221.602902 109.02986 \nL 221.634604 102.026663 \nL 221.67159 103.970331 \nL 221.676874 102.393675 \nL 221.761413 109.576634 \nL 221.798399 110.595281 \nL 221.835384 107.344599 \nL 221.87237 109.894961 \nL 221.877654 109.936156 \nL 221.993895 121.837847 \nL 222.120704 102.124033 \nL 222.125987 101.895587 \nL 222.131271 104.61822 \nL 222.152406 103.696944 \nL 222.157689 109.778865 \nL 243.007119 113.763572 \nL 243.017687 107.62173 \nL 243.091658 124.215938 \nL 243.107509 118.785651 \nL 243.223751 103.108975 \nL 243.255453 104.962762 \nL 243.260736 106.378382 \nL 243.318857 93.843782 \nL 243.324141 95.124581 \nL 243.350559 96.910958 \nL 243.387545 90.008876 \nL 243.392829 89.229911 \nL 243.482651 92.154776 \nL 243.630595 100.023074 \nL 243.651729 96.798607 \nL 243.752119 94.678923 \nL 243.699282 100.86945 \nL 243.762687 95.809921 \nL 243.799673 99.116778 \nL 243.815524 94.869919 \nL 243.878928 97.95582 \nL 243.958183 94.491672 \nL 243.989885 95.933506 \nL 244.000453 97.064504 \nL 244.048006 88.787998 \nL 244.132545 85.679627 \nL 244.095559 91.349595 \nL 244.164247 87.211342 \nL 244.275204 91.196049 \nL 244.2118 84.810781 \nL 244.280488 90.776607 \nL 244.301623 87.132697 \nL 244.359743 94.731353 \nL 244.391445 90.536925 \nL 244.470701 93.918683 \nL 244.502403 91.079954 \nL 244.55524 82.541296 \nL 244.618644 85.114128 \nL 244.692616 88.611982 \nL 244.676765 84.286478 \nL 244.734885 87.555885 \nL 244.745453 89.252381 \nL 244.787722 86.364967 \nL 244.819424 88.196284 \nL 244.909247 77.320731 \nL 244.940949 80.500257 \nL 244.972651 84.335163 \nL 244.951516 80.290536 \nL 244.983218 80.92719 \nL 250.610345 77.683998 \nL 250.63148 79.833643 \nL 250.668466 68.695937 \nL 250.705452 67.078086 \nL 250.758289 72.605744 \nL 250.768856 70.980403 \nL 250.943218 93.918683 \nL 250.996055 87.784331 \nL 250.964352 98.255422 \nL 251.054175 93.866252 \nL 251.101728 90.027601 \nL 251.085877 95.109601 \nL 251.122863 94.180835 \nL 251.233821 100.693434 \nL 251.170416 93.102267 \nL 251.244388 97.952075 \nL 251.323643 93.087287 \nL 251.27609 98.390243 \nL 251.355345 97.19558 \nL 251.365913 97.96331 \nL 251.429317 101.483634 \nL 251.455735 97.719883 \nL 251.47687 98.75351 \nL 251.487438 100.045544 \nL 251.503289 97.098209 \nL 251.57726 91.420751 \nL 251.614246 96.113268 \nL 251.751622 88.533336 \nL 251.635381 98.116856 \nL 251.756906 89.465848 \nL 251.830877 93.622826 \nL 251.867863 91.057483 \nL 251.936551 81.781056 \nL 252.031657 84.499944 \nL 252.153182 97.813509 \nL 252.089778 80.477787 \nL 252.232437 96.240598 \nL 252.274707 88.132619 \nL 252.343395 92.982426 \nL 252.475487 99.659807 \nL 252.369813 92.510553 \nL 252.480771 97.96331 \nL 252.570593 93.656531 \nL 252.591728 94.581552 \nL 258.218855 106.412087 \nL 258.224139 102.577181 \nL 258.292827 112.291777 \nL 258.329813 105.412165 \nL 258.350947 102.929214 \nL 258.377366 108.393206 \nL 258.419635 108.32205 \nL 258.514742 113.654966 \nL 258.551727 111.22819 \nL 258.662685 106.370892 \nL 258.567578 112.617594 \nL 258.694387 108.745238 \nL 258.720805 110.164603 \nL 258.778926 107.808982 \nL 258.78421 107.857667 \nL 258.8846 104.146347 \nL 258.900451 106.258541 \nL 259.111798 112.613849 \nL 259.138217 109.82006 \nL 259.270309 105.940214 \nL 259.169919 110.55783 \nL 259.291444 106.329697 \nL 259.344281 105.693042 \nL 259.402401 108.243405 \nL 259.407685 107.022526 \nL 259.439387 111.078389 \nL 259.508075 108.797668 \nL 259.518642 108.464361 \nL 259.582046 104.146347 \nL 259.634883 106.674239 \nL 259.640167 106.677984 \nL 259.724706 100.157895 \nL 259.751124 104.367303 \nL 259.756408 106.464518 \nL 259.851515 102.333755 \nL 259.856798 102.697022 \nL 259.936054 105.045153 \nL 259.883217 101.61471 \nL 259.967756 103.056545 \nL 259.973039 103.037819 \nL 260.078713 92.656609 \nL 260.099848 93.903703 \nL 260.136834 90.390869 \nL 260.115699 93.989838 \nL 260.200238 90.450789 \nL 265.827365 84.425044 \nL 265.874918 77.272045 \nL 265.94889 77.534197 \nL 266.070414 84.930622 \nL 266.091549 80.717469 \nL 266.191939 77.257065 \nL 266.117968 82.676117 \nL 266.197223 79.893563 \nL 266.324031 84.001856 \nL 266.361017 82.376515 \nL 266.40857 85.365045 \nL 266.429705 84.016836 \nL 266.466691 86.024169 \nL 266.519528 83.020659 \nL 266.540663 84.080501 \nL 266.545946 84.979307 \nL 266.619918 78.949817 \nL 266.667471 77.376906 \nL 266.69389 80.002169 \nL 266.725592 78.459218 \nL 266.825982 81.230537 \nL 266.741443 77.148459 \nL 266.847116 80.429102 \nL 266.857684 79.766232 \nL 266.915804 81.683685 \nL 266.942223 84.177872 \nL 266.968641 79.627666 \nL 267.021478 80.9197 \nL 267.032046 80.073324 \nL 267.063748 82.346555 \nL 267.079599 82.054443 \nL 267.15357 87.791821 \nL 267.179989 81.136911 \nL 267.185272 81.537629 \nL 267.190556 81.635 \nL 267.19584 80.387906 \nL 267.216975 80.432847 \nL 267.333216 71.639527 \nL 267.35435 73.324789 \nL 267.391336 67.048126 \nL 267.407187 68.265259 \nL 267.417755 67.216652 \nL 267.475875 71.508451 \nL 267.507577 68.755858 \nL 267.613251 76.062402 \nL 267.523428 67.958167 \nL 267.644953 72.879131 \nL 267.660804 69.441197 \nL 267.734776 72.714349 \nL 267.74006 74.785348 \nL 267.803464 71.478491 \nL 267.808748 71.478491 \nL 273.435875 76.298339 \nL 273.47286 79.47412 \nL 273.562683 62.636486 \nL 273.663073 72.972756 \nL 273.689492 65.868443 \nL 273.700059 60.636643 \nL 273.758179 72.425982 \nL 273.784598 70.441119 \nL 274.101619 94.364341 \nL 273.8163 67.501274 \nL 274.122754 93.031112 \nL 274.21786 89.03517 \nL 274.238995 89.428397 \nL 274.244279 88.810468 \nL 274.291832 92.592944 \nL 274.323534 91.604257 \nL 274.36052 94.637728 \nL 274.413357 90.814057 \nL 274.429208 91.469436 \nL 274.556016 84.747116 \nL 274.482045 91.903859 \nL 274.582435 87.514689 \nL 274.714527 95.76498 \nL 274.719811 94.907369 \nL 274.878321 106.11623 \nL 274.883605 104.842922 \nL 274.894172 106.232326 \nL 275.031548 98.480124 \nL 275.100236 97.809764 \nL 275.158357 102.550966 \nL 275.232328 96.053347 \nL 275.211193 102.989134 \nL 275.279881 99.813353 \nL 275.285165 102.142758 \nL 275.380271 95.731275 \nL 275.385555 95.959722 \nL 275.401406 94.574062 \nL 275.417257 94.645218 \nL 296.261404 90.473259 \nL 296.330092 87.679471 \nL 296.303673 92.192226 \nL 296.367077 89.986406 \nL 296.39878 94.113424 \nL 296.472751 87.64202 \nL 296.546723 83.777154 \nL 296.594276 86.496043 \nL 296.641829 87.62704 \nL 296.715801 84.983052 \nL 296.736936 87.211342 \nL 296.789772 82.691097 \nL 296.80034 82.942014 \nL 296.805623 83.076834 \nL 296.837326 81.065756 \nL 296.85846 82.616196 \nL 296.863744 81.691175 \nL 296.911297 85.454926 \nL 296.948283 84.777076 \nL 296.979985 86.818115 \nL 297.032822 83.814604 \nL 297.053957 84.413809 \nL 297.05924 84.376358 \nL 297.064524 85.132853 \nL 297.164914 86.466082 \nL 297.096226 83.95317 \nL 297.175482 85.979229 \nL 297.238886 83.9307 \nL 297.30229 83.979386 \nL 297.418531 91.390791 \nL 297.318141 82.537551 \nL 297.481935 87.904172 \nL 297.582325 85.320105 \nL 297.614028 85.878113 \nL 297.619311 86.102815 \nL 297.640446 83.252851 \nL 297.682716 80.90472 \nL 297.735552 84.964327 \nL 297.740836 84.473729 \nL 297.74612 85.780743 \nL 297.825375 83.121775 \nL 297.851794 84.488709 \nL 297.968035 80.107029 \nL 298.078992 85.189029 \nL 298.100127 83.069344 \nL 298.105411 82.619941 \nL 298.14768 88.26744 \nL 298.152964 87.368633 \nL 298.237503 90.184893 \nL 298.195233 86.526003 \nL 298.242786 89.585688 \nL 303.875197 94.663943 \nL 303.885764 93.956133 \nL 303.901615 97.221795 \nL 304.208069 117.276406 \nL 304.245055 113.029547 \nL 304.313743 119.527166 \nL 304.435268 113.265484 \nL 304.440552 113.430265 \nL 304.519807 112.213131 \nL 304.482821 114.482617 \nL 304.535658 114.16429 \nL 304.551509 116.576086 \nL 304.604346 111.999665 \nL 304.60963 111.104604 \nL 304.688885 116.134173 \nL 304.704736 115.681025 \nL 304.731154 117.710829 \nL 304.794559 122.28725 \nL 304.836828 117.699594 \nL 304.963637 114.205485 \nL 305.01119 115.295287 \nL 305.027041 116.48246 \nL 305.074594 113.280464 \nL 305.090445 114.385246 \nL 305.238388 106.262286 \nL 305.243672 106.659259 \nL 305.391615 117.381266 \nL 305.396899 117.029234 \nL 305.486722 111.932255 \nL 305.544842 113.396559 \nL 305.571261 115.381423 \nL 305.639949 110.673926 \nL 305.645232 108.850099 \nL 305.724488 115.018155 \nL 305.740339 113.864688 \nL 305.793176 116.995529 \nL 305.824878 113.452735 \nL 305.851296 114.029469 \nL 311.478423 117.426207 \nL 311.483707 117.134095 \nL 311.499558 119.894178 \nL 311.510125 125.99108 \nL 311.578813 102.502281 \nL 311.584097 103.10523 \nL 311.726756 91.525612 \nL 311.621083 103.408577 \nL 311.73204 91.581787 \nL 311.869416 98.232952 \nL 311.747891 91.053738 \nL 311.890551 97.064504 \nL 311.927536 96.117013 \nL 311.953955 99.184188 \nL 311.990941 98.326578 \nL 312.001508 95.379243 \nL 312.03321 99.48379 \nL 312.117749 97.244265 \nL 312.128317 97.9708 \nL 312.170586 95.806176 \nL 312.228707 97.596298 \nL 312.313246 101.285148 \nL 312.408352 100.719649 \nL 312.450622 96.337969 \nL 312.440054 101.285148 \nL 312.524593 99.154228 \nL 312.640834 104.318618 \nL 312.651402 103.288736 \nL 312.656685 102.629612 \nL 312.709522 108.711533 \nL 312.77821 136.113883 \nL 312.889168 125.721438 \nL 312.894451 120.077685 \nL 312.968423 127.972198 \nL 313.000125 124.485579 \nL 313.111082 133.541051 \nL 313.02126 123.260956 \nL 313.142785 133.252684 \nL 313.148068 132.428778 \nL 313.206189 139.851418 \nL 313.227324 145.510152 \nL 313.248458 135.645755 \nL 313.306579 139.102413 \nL 313.391118 132.177861 \nL 313.317146 139.480661 \nL 313.433387 134.750694 \nL 313.449238 133.080413 \nL 313.459806 133.990454 \nL 319.086933 131.773399 \nL 319.145053 137.357231 \nL 319.129202 128.788614 \nL 319.171472 128.923434 \nL 319.213741 124.257133 \nL 319.282429 128.96463 \nL 319.361685 135.121451 \nL 319.319415 127.814907 \nL 319.419805 133.361289 \nL 319.446223 129.365347 \nL 319.493777 127.560245 \nL 319.514911 130.676106 \nL 319.562465 127.623911 \nL 319.69984 134.8593 \nL 319.64172 126.125901 \nL 319.705124 133.956748 \nL 319.773812 131.23037 \nL 319.831933 131.661048 \nL 319.847784 131.088059 \nL 319.858351 133.346309 \nL 319.863635 132.454993 \nL 319.879486 126.81124 \nL 319.974592 128.766143 \nL 319.979876 129.125666 \nL 320.001011 126.545343 \nL 320.080266 128.758653 \nL 320.08555 127.983433 \nL 320.111968 133.072922 \nL 320.175372 130.47762 \nL 320.18594 135.379858 \nL 320.281046 129.174351 \nL 320.402571 119.077763 \nL 320.333883 130.402719 \nL 320.434273 119.811788 \nL 320.508245 124.006216 \nL 320.529379 119.092743 \nL 320.54523 120.897845 \nL 320.561082 120.014019 \nL 320.629769 122.613067 \nL 320.635053 122.568126 \nL 320.640337 123.193546 \nL 320.777713 111.224445 \nL 320.878103 115.980627 \nL 320.78828 108.501811 \nL 320.920372 114.19425 \nL 321.041897 108.621652 \nL 321.063032 109.438068 \nL 321.068315 108.954959 \nL 326.695442 97.16562 \nL 326.769414 102.547221 \nL 326.732428 96.51773 \nL 326.8064 97.341636 \nL 326.811684 95.907291 \nL 326.869804 104.809216 \nL 326.890939 103.962841 \nL 326.901506 105.352245 \nL 326.922641 102.341245 \nL 326.933208 103.861725 \nL 326.938492 102.243874 \nL 327.00718 108.048663 \nL 327.033598 106.490733 \nL 327.081152 103.599573 \nL 327.091719 106.670494 \nL 327.144556 106.273521 \nL 327.170974 108.610417 \nL 327.244946 106.067545 \nL 327.25023 105.329775 \nL 327.30835 112.231857 \nL 327.313634 110.995998 \nL 327.424591 120.493382 \nL 327.456293 117.89059 \nL 327.482712 116.632261 \nL 327.514414 118.373698 \nL 327.519698 119.074018 \nL 327.598953 115.00692 \nL 327.614804 116.943098 \nL 327.630655 125.039843 \nL 327.725761 122.583107 \nL 327.746896 124.193467 \nL 327.805017 121.174977 \nL 327.8103 121.54199 \nL 327.958244 117.257681 \nL 327.894839 122.231074 \nL 327.963527 117.497362 \nL 328.037499 122.485736 \nL 328.079769 120.115135 \nL 328.111471 120.969001 \nL 328.148456 118.744456 \nL 328.19601 120.706849 \nL 328.359804 112.494008 \nL 328.391506 117.800709 \nL 328.486612 115.924452 \nL 328.518315 115.171701 \nL 328.5553 117.426207 \nL 328.565868 118.868041 \nL 328.65569 115.681025 \nL 328.676825 116.823258 \nL 349.520972 131.795869 \nL 349.610794 163.568664 \nL 349.536823 129.634989 \nL 349.727035 150.483545 \nL 349.827425 141.042336 \nL 349.89083 142.802498 \nL 350.001787 146.510073 \nL 349.959518 142.798753 \nL 350.007071 145.24051 \nL 350.017638 143.821145 \nL 350.081042 149.375018 \nL 350.096893 148.899399 \nL 350.128596 152.269922 \nL 350.176149 148.315176 \nL 350.207851 151.176375 \nL 350.297674 146.360272 \nL 350.329376 146.371507 \nL 350.445617 147.629836 \nL 350.371645 143.353017 \nL 350.450901 147.569915 \nL 350.49317 147.873263 \nL 350.509021 145.375331 \nL 350.519588 145.768558 \nL 350.59356 144.341704 \nL 350.609411 146.232941 \nL 350.614695 146.281627 \nL 350.625262 145.536367 \nL 350.667532 142.802498 \nL 350.741503 144.873497 \nL 350.746787 144.701226 \nL 350.767922 147.075572 \nL 350.783773 148.142904 \nL 350.820759 145.850949 \nL 350.841893 131.619853 \nL 350.937 135.503444 \nL 350.99512 130.657381 \nL 351.047957 134.110294 \nL 351.053241 134.570933 \nL 351.084943 130.009492 \nL 351.116645 130.829652 \nL 351.153631 132.971807 \nL 351.222319 128.631322 \nL 351.227603 131.739693 \nL 351.317425 121.819121 \nL 351.333276 121.212427 \nL 351.33856 122.766613 \nL 351.349127 122.564381 \nL 351.38083 126.504148 \nL 351.444234 122.526931 \nL 351.460085 123.590518 \nL 351.497071 125.287015 \nL 351.470652 123.253466 \nL 351.502354 124.863827 \nL 357.129481 113.070742 \nL 357.224588 99.43885 \nL 357.31441 103.700689 \nL 357.404233 104.895352 \nL 357.367247 101.371283 \nL 357.425368 104.221247 \nL 357.435935 102.221404 \nL 357.504623 106.767865 \nL 357.520474 105.251129 \nL 357.641999 109.894961 \nL 357.65785 108.116074 \nL 357.705403 103.487222 \nL 357.774091 106.7791 \nL 357.863914 104.000291 \nL 357.821644 107.044997 \nL 357.9009 105.940214 \nL 357.932602 108.411931 \nL 357.964304 105.483321 \nL 358.011857 107.595515 \nL 358.128098 98.109366 \nL 358.096396 107.838942 \nL 358.191502 98.854626 \nL 358.217921 97.588808 \nL 358.36058 103.007859 \nL 358.429268 104.745551 \nL 358.450403 101.66714 \nL 358.471538 103.374872 \nL 358.508524 109.561654 \nL 358.598346 105.528261 \nL 358.6459 99.936938 \nL 358.651183 98.922037 \nL 358.725155 102.281324 \nL 358.741006 102.064113 \nL 358.74629 102.037898 \nL 358.767424 100.637258 \nL 358.867814 104.296148 \nL 358.957637 100.165385 \nL 358.989339 100.865705 \nL 359.026325 100.708414 \nL 359.110864 104.067701 \nL 364.737991 111.100859 \nL 364.743275 112.149466 \nL 364.78026 102.708257 \nL 364.854232 93.016132 \nL 364.896502 96.296774 \nL 365.012743 106.722924 \nL 365.018026 106.344677 \nL 365.128984 110.153368 \nL 365.070863 103.719414 \nL 365.144835 108.34452 \nL 365.150119 106.513203 \nL 365.239941 111.149544 \nL 365.26636 114.486362 \nL 365.319197 109.033605 \nL 365.350899 111.314325 \nL 365.414303 115.149231 \nL 365.461856 111.673848 \nL 365.472424 110.943568 \nL 365.509409 113.276719 \nL 365.535828 115.433853 \nL 365.56753 111.662613 \nL 365.620367 113.381579 \nL 365.752459 117.291386 \nL 365.636218 110.610261 \nL 365.76831 117.186525 \nL 365.81058 114.482617 \nL 365.815863 120.276171 \nL 365.926821 123.470678 \nL 365.932104 122.759123 \nL 366.058913 117.752024 \nL 365.96909 123.478168 \nL 366.101182 119.897923 \nL 366.11175 121.673065 \nL 366.180438 117.411227 \nL 366.206856 118.523499 \nL 366.21214 119.568361 \nL 366.301962 116.212818 \nL 366.31253 116.800787 \nL 366.323097 115.613614 \nL 366.328381 120.665654 \nL 366.397069 113.610026 \nL 366.434055 115.516244 \nL 366.618984 121.006451 \nL 366.508026 114.493852 \nL 366.640118 119.163898 \nL 366.645402 118.216407 \nL 366.666537 122.152429 \nL 366.719374 121.474579 \nL 372.346501 142.708873 \nL 372.362352 144.581385 \nL 372.420472 137.203685 \nL 372.446891 139.106158 \nL 372.552564 134.125275 \nL 372.457458 139.514366 \nL 372.605401 134.90424 \nL 372.610685 136.267429 \nL 372.663522 132.69093 \nL 372.711075 133.934278 \nL 372.806181 129.983277 \nL 372.837884 132.881926 \nL 373.001678 137.143765 \nL 373.054515 136.533326 \nL 373.07565 138.836516 \nL 373.091501 137.997631 \nL 373.102068 139.098668 \nL 373.128486 136.945279 \nL 373.197174 138.061296 \nL 373.202458 137.960181 \nL 373.213025 139.521856 \nL 373.308132 143.682579 \nL 373.244728 139.458191 \nL 373.329267 142.012298 \nL 373.360969 143.13206 \nL 373.593451 133.930533 \nL 373.688557 142.120904 \nL 373.709692 138.671735 \nL 373.746678 136.907828 \nL 373.783664 139.248469 \nL 373.799515 138.297233 \nL 373.942174 155.093671 \nL 374.026713 151.9104 \nL 373.973876 159.827383 \nL 374.042564 156.183474 \nL 374.068983 160.512723 \nL 374.132387 151.296216 \nL 374.243344 144.098277 \nL 374.259195 146.457643 \nL 374.317316 156.524271 \nL 374.327883 155.973752 \nL 379.95501 140.81389 \nL 379.960294 137.229901 \nL 379.991996 153.872793 \nL 380.050117 150.333744 \nL 380.0554 149.494859 \nL 380.102954 159.190729 \nL 380.176925 175.069636 \nL 380.113521 159.160769 \nL 380.245613 172.743976 \nL 380.266748 168.16381 \nL 380.335436 176.796093 \nL 380.356571 183.540884 \nL 380.430542 175.904777 \nL 380.451677 182.664548 \nL 380.456961 180.522393 \nL 380.504514 198.034132 \nL 380.5415 189.270772 \nL 380.562634 191.07962 \nL 380.59962 184.323594 \nL 380.620755 185.188695 \nL 380.636606 183.413553 \nL 380.678876 190.971014 \nL 380.705294 188.18846 \nL 380.710578 190.083443 \nL 380.789833 185.881524 \nL 380.810968 186.990052 \nL 380.832102 180.286457 \nL 380.884939 189.521689 \nL 380.927209 183.439768 \nL 380.932493 184.080167 \nL 381.006464 180.357612 \nL 381.048734 176.133224 \nL 381.112138 182.016658 \nL 381.212528 185.334751 \nL 381.201961 180.511158 \nL 381.223095 182.672038 \nL 381.265365 179.571157 \nL 381.312918 185.821604 \nL 381.323485 185.083834 \nL 381.371039 180.275222 \nL 381.471429 168.18628 \nL 381.524266 169.452098 \nL 381.577102 173.515451 \nL 381.608805 168.90907 \nL 381.698627 155.048731 \nL 381.725046 156.943714 \nL 381.783166 166.077831 \nL 381.825436 156.014948 \nL 381.936393 142.263215 \nL 402.780539 192.195637 \nL 402.791107 196.206559 \nL 402.849227 181.200243 \nL 402.859795 182.758173 \nL 402.923199 174.021029 \nL 402.970752 180.777055 \nL 403.002454 179.421356 \nL 402.98132 181.51108 \nL 403.013022 179.994345 \nL 403.071142 169.725485 \nL 403.129263 172.358238 \nL 403.150398 173.309475 \nL 403.171532 168.699348 \nL 403.256071 166.444843 \nL 403.219086 171.702859 \nL 403.277206 168.972735 \nL 403.314192 167.373609 \nL 403.404015 174.425492 \nL 403.54139 171.039989 \nL 403.414582 175.215692 \nL 403.546674 171.066204 \nL 403.551958 171.691624 \nL 403.636497 168.601978 \nL 403.641781 168.538312 \nL 403.742171 173.178399 \nL 403.763305 172.305808 \nL 403.921816 161.669936 \nL 403.9271 162.415196 \nL 403.942951 162.501331 \nL 403.990504 158.677661 \nL 404.101461 146.217961 \nL 404.117312 147.951908 \nL 404.191284 144.757401 \nL 404.138447 149.67462 \nL 404.201851 146.656129 \nL 404.238837 154.479487 \nL 404.307525 147.146728 \nL 404.418483 139.795243 \nL 404.32866 148.487447 \nL 404.42905 140.731499 \nL 404.434334 140.967436 \nL 404.444901 137.948945 \nL 404.476603 140.166 \nL 404.524156 131.904475 \nL 404.598128 134.829339 \nL 404.608695 137.244881 \nL 404.650965 129.440248 \nL 404.687951 130.380249 \nL 404.714369 128.33172 \nL 404.751355 130.4177 \nL 404.761922 129.912121 \nL 410.389049 115.493774 \nL 410.394333 116.991784 \nL 410.426035 105.861569 \nL 410.484156 112.295522 \nL 410.542276 104.532085 \nL 410.558127 103.153915 \nL 410.626815 109.539183 \nL 410.632099 109.557909 \nL 410.769475 99.779647 \nL 410.817028 103.479732 \nL 410.885716 107.288423 \nL 410.927985 104.917822 \nL 410.954404 103.689454 \nL 410.99139 107.730336 \nL 411.097063 114.044449 \nL 411.012524 107.588025 \nL 411.123482 113.194328 \nL 411.181602 109.078545 \nL 411.186886 109.82006 \nL 411.213304 105.561966 \nL 411.276709 110.464205 \nL 411.297843 108.453126 \nL 411.419368 118.901747 \nL 411.429936 118.497284 \nL 411.461638 114.770984 \nL 411.503907 121.159997 \nL 411.519758 119.193859 \nL 411.630716 122.470756 \nL 411.583163 115.894491 \nL 411.635999 122.036333 \nL 411.736389 118.624615 \nL 411.709971 123.650439 \nL 411.746957 119.871708 \nL 411.8949 133.084158 \nL 411.900184 131.548697 \nL 411.958304 126.511638 \nL 411.931886 132.162881 \nL 412.021709 127.417934 \nL 412.026992 128.208135 \nL 412.048127 120.145095 \nL 412.106248 124.028686 \nL 412.111531 123.927571 \nL 412.116815 125.500481 \nL 412.322879 144.135727 \nL 412.328162 142.222019 \nL 412.354581 142.926084 \nL 412.370432 141.304488 \nL 417.997559 127.462875 \nL 418.039828 139.607992 \nL 418.129651 133.878103 \nL 418.134935 132.799536 \nL 418.198339 141.484249 \nL 418.208906 139.960024 \nL 418.319864 148.558602 \nL 418.245892 136.799223 \nL 418.362133 145.075729 \nL 418.372701 144.993338 \nL 418.383268 147.772147 \nL 418.425538 141.869987 \nL 418.473091 142.270705 \nL 418.525928 136.391015 \nL 418.573481 142.600267 \nL 418.636885 138.428309 \nL 418.647452 141.016121 \nL 418.71614 132.657225 \nL 418.726708 134.323761 \nL 418.731991 133.297624 \nL 418.800679 140.641619 \nL 418.874651 144.603855 \nL 418.890502 138.68297 \nL 418.901069 139.963769 \nL 418.975041 136.244959 \nL 418.927488 140.10608 \nL 419.017311 139.319625 \nL 419.075431 137.829105 \nL 419.096566 140.289586 \nL 419.107133 142.495406 \nL 419.138835 137.585678 \nL 419.207523 141.106002 \nL 419.223374 138.930142 \nL 419.265644 144.012141 \nL 419.270928 146.989437 \nL 419.36075 136.45468 \nL 419.413587 141.308233 \nL 419.450573 135.600815 \nL 419.455857 135.949102 \nL 419.535112 130.163038 \nL 419.476991 136.698107 \nL 419.566814 135.934122 \nL 419.656637 141.244568 \nL 419.688339 139.416995 \nL 419.693623 139.023768 \nL 419.698906 142.139629 \nL 419.815147 153.580681 \nL 419.825715 154.344666 \nL 419.873268 155.408253 \nL 419.94724 149.577249 \nL 419.957807 151.479722 \nL 419.978942 150.992869 \nL 425.606069 145.843459 \nL 425.727593 137.761694 \nL 425.764579 134.18894 \nL 425.817416 138.784086 \nL 425.838551 137.244881 \nL 425.849118 138.559385 \nL 425.907239 141.592855 \nL 425.965359 140.169745 \nL 425.991778 136.391015 \nL 426.0816 138.971337 \nL 426.092168 139.3009 \nL 426.145005 140.630384 \nL 426.176707 138.312213 \nL 426.197842 138.634285 \nL 426.372203 144.618836 \nL 426.403905 142.413016 \nL 426.430324 145.120669 \nL 426.488444 143.476603 \nL 426.52543 145.450231 \nL 426.535998 142.132139 \nL 426.541281 138.095001 \nL 426.636388 142.330625 \nL 426.646955 141.162177 \nL 426.652239 141.162177 \nL 426.657522 141.555405 \nL 426.710359 139.484406 \nL 426.763196 141.154687 \nL 426.884721 131.118019 \nL 426.916423 133.211488 \nL 426.92699 134.301291 \nL 427.000962 128.429091 \nL 427.006246 128.380406 \nL 427.096068 126.878651 \nL 427.059083 131.627343 \nL 427.117203 128.298015 \nL 427.244012 138.62305 \nL 427.159473 125.234584 \nL 427.27043 131.372681 \nL 427.286281 127.983433 \nL 427.354969 131.586147 \nL 427.386671 128.178174 \nL 427.434224 136.057708 \nL 427.508196 132.930612 \nL 427.529331 134.072844 \nL 427.539898 134.604638 \nL 427.5716 130.683596 \nL 427.587451 130.956983 \nL 433.214578 123.83769 \nL 433.336103 111.010978 \nL 433.373089 109.303247 \nL 433.394224 113.572576 \nL 433.410075 112.913451 \nL 433.48933 119.32868 \nL 433.526316 115.950667 \nL 433.563302 118.789396 \nL 433.616139 114.699828 \nL 433.63199 115.512499 \nL 433.642557 114.063174 \nL 433.69011 118.549714 \nL 433.721812 121.991393 \nL 433.801068 119.261269 \nL 433.938443 113.984528 \nL 433.827486 120.463422 \nL 433.964862 114.044449 \nL 433.980713 115.471303 \nL 433.996564 112.816081 \nL 434.186777 102.165229 \nL 434.207912 103.487222 \nL 434.255465 105.427146 \nL 434.308302 101.064191 \nL 434.318869 100.16913 \nL 434.387557 103.419812 \nL 434.413975 100.91439 \nL 434.440394 103.404832 \nL 434.461529 99.397655 \nL 434.503798 99.577416 \nL 434.672876 88.847918 \nL 434.5355 101.176542 \nL 434.683443 89.75796 \nL 434.730997 87.009111 \nL 434.799685 84.005601 \nL 434.83667 87.372379 \nL 434.847238 89.14003 \nL 434.910642 84.207832 \nL 434.952911 88.316125 \nL 434.958195 87.510944 \nL 434.989897 91.784018 \nL 435.042734 91.499396 \nL 435.048018 91.495651 \nL 435.111422 90.596845 \nL 435.090287 93.128482 \nL 435.13784 91.97127 \nL 435.185394 96.914703 \nL 435.195961 95.349282 \nL 456.040107 83.664803 \nL 456.045391 78.373083 \nL 456.077093 86.600903 \nL 456.145781 86.387437 \nL 456.151065 86.462337 \nL 456.203902 97.91837 \nL 456.267306 94.457966 \nL 456.299008 90.256048 \nL 456.283157 97.465222 \nL 456.415249 90.858997 \nL 456.515639 100.95933 \nL 456.547341 98.506339 \nL 456.563192 101.247697 \nL 456.616029 97.229285 \nL 456.668866 100.244031 \nL 456.795675 92.386967 \nL 456.816809 93.087287 \nL 456.922483 97.468967 \nL 456.869646 91.495651 \nL 456.943618 95.918526 \nL 456.985887 94.158364 \nL 456.959469 97.772314 \nL 457.038724 95.862351 \nL 457.065143 98.262912 \nL 457.133831 94.738843 \nL 457.144398 95.603944 \nL 457.223653 93.165933 \nL 457.186668 96.862273 \nL 457.255356 94.248245 \nL 457.265923 95.199481 \nL 457.313476 87.799311 \nL 457.334611 90.154932 \nL 457.339894 89.503298 \nL 457.387448 94.420516 \nL 457.398015 93.450555 \nL 457.482554 96.704982 \nL 457.514256 95.873586 \nL 457.545958 97.622513 \nL 457.672767 89.13254 \nL 457.704469 88.585767 \nL 457.736171 93.866252 \nL 457.741455 95.596454 \nL 457.825994 91.342105 \nL 457.841845 92.136051 \nL 457.889398 99.817098 \nL 457.931667 91.29342 \nL 457.968653 94.057249 \nL 457.984504 90.042582 \nL 458.02149 90.207363 \nL 463.648617 99.236619 \nL 463.717305 109.134721 \nL 463.764858 103.296226 \nL 463.770142 102.929214 \nL 463.817695 107.221013 \nL 463.912801 113.47146 \nL 463.918085 114.19425 \nL 463.944504 109.557909 \nL 464.007908 112.209386 \nL 464.013192 111.726278 \nL 464.092447 113.535125 \nL 464.198121 116.549871 \nL 464.176986 113.385324 \nL 464.203404 115.486284 \nL 464.335496 110.715121 \nL 464.515142 115.763415 \nL 464.525709 114.97696 \nL 464.557411 113.505165 \nL 464.599681 116.059272 \nL 464.620816 115.167956 \nL 464.647234 115.965647 \nL 464.705355 112.677515 \nL 464.715922 112.06333 \nL 464.726489 113.639986 \nL 464.800461 110.636476 \nL 464.821596 110.718866 \nL 464.911418 114.898315 \nL 464.948404 112.909706 \nL 464.964255 114.280385 \nL 464.990674 112.512733 \nL 465.059362 108.003723 \nL 465.101631 110.085957 \nL 465.106915 110.082212 \nL 465.112199 109.546673 \nL 465.143901 113.276719 \nL 465.149184 113.175603 \nL 465.159752 123.83769 \nL 465.265425 117.692103 \nL 465.371099 125.223349 \nL 465.291844 117.673378 \nL 465.418652 124.249643 \nL 465.540177 119.107723 \nL 465.577163 119.632027 \nL 465.58773 118.456089 \nL 465.593014 116.609791 \nL 465.63 120.527088 \nL 471.257127 123.706614 \nL 471.272978 114.14931 \nL 471.34695 126.04351 \nL 471.362801 123.998726 \nL 471.40507 127.597695 \nL 471.389219 122.736653 \nL 471.452623 122.755378 \nL 471.574148 117.092899 \nL 471.468474 122.811553 \nL 471.60585 120.587008 \nL 471.621701 122.474501 \nL 471.674538 119.059038 \nL 471.827765 105.37846 \nL 471.838332 104.887862 \nL 471.843616 106.247306 \nL 471.94929 108.632887 \nL 471.864751 105.101328 \nL 471.970425 108.385716 \nL 472.002127 107.666671 \nL 472.039113 110.906118 \nL 472.102517 108.595437 \nL 472.160637 113.651221 \nL 472.181772 111.651378 \nL 472.245176 114.25417 \nL 472.266311 113.336639 \nL 472.366701 117.650908 \nL 472.298013 112.887236 \nL 472.39312 116.089233 \nL 472.498793 112.381658 \nL 472.519928 114.25417 \nL 472.609751 116.018077 \nL 472.556914 111.838629 \nL 472.636169 115.321502 \nL 472.641453 115.332738 \nL 472.778829 125.953629 \nL 472.826382 123.332112 \nL 472.858084 121.047646 \nL 472.905637 125.37315 \nL 472.99546 134.252605 \nL 473.027162 130.874593 \nL 473.043013 130.44766 \nL 473.180389 140.244646 \nL 473.233226 136.810458 \nL 473.23851 137.536993 \nL 478.865637 133.818183 \nL 478.87092 133.218978 \nL 478.897339 140.139785 \nL 478.923757 137.81038 \nL 478.950176 142.731343 \nL 479.018863 134.469817 \nL 479.024147 135.151412 \nL 479.066417 132.237782 \nL 479.11397 136.686872 \nL 479.182658 139.903849 \nL 479.224927 138.634285 \nL 479.246062 135.71691 \nL 479.309466 142.446721 \nL 479.325317 141.5217 \nL 479.341168 140.738989 \nL 479.378154 142.918594 \nL 479.383438 144.293018 \nL 479.441558 136.42472 \nL 479.467977 138.136197 \nL 479.510246 135.720655 \nL 479.541949 140.102335 \nL 479.573651 139.671657 \nL 479.626488 141.791341 \nL 479.647622 137.82536 \nL 479.674041 139.240979 \nL 479.71631 137.08759 \nL 479.737445 139.40576 \nL 479.779714 138.858987 \nL 479.827268 137.713009 \nL 479.922374 142.723853 \nL 480.01748 132.503679 \nL 480.038615 133.37627 \nL 480.075601 135.391093 \nL 480.107303 133.009257 \nL 480.186558 127.695066 \nL 480.223544 129.608774 \nL 480.382055 138.844006 \nL 480.26053 129.155626 \nL 480.413757 135.791811 \nL 480.419041 132.915631 \nL 480.493012 137.256116 \nL 480.519431 135.60456 \nL 480.714927 145.480191 \nL 480.720211 145.169354 \nL 480.783615 145.656208 \nL 480.836452 141.562895 \nL 480.841736 140.877555 \nL 480.847019 141.248313 \nL 486.474146 139.057473 \nL 486.47943 139.173569 \nL 486.484714 138.020101 \nL 486.53755 147.116767 \nL 486.574536 141.450544 \nL 486.659075 146.315332 \nL 486.669643 142.641462 \nL 486.775316 136.301134 \nL 486.785884 136.619461 \nL 486.801735 139.667912 \nL 486.865139 135.623285 \nL 486.902125 137.8703 \nL 486.917976 136.158823 \nL 486.970813 143.330547 \nL 486.98138 144.469035 \nL 487.060636 140.690304 \nL 487.219146 137.477072 \nL 487.08177 142.398036 \nL 487.229714 138.121217 \nL 487.234997 140.267116 \nL 487.335387 137.342251 \nL 487.441061 134.207665 \nL 487.467479 135.589579 \nL 487.472763 135.859221 \nL 487.515033 132.994277 \nL 487.557302 134.784399 \nL 487.594288 133.009257 \nL 487.636557 134.578423 \nL 487.641841 135.66448 \nL 487.689394 129.354112 \nL 487.731664 131.979375 \nL 487.742231 131.852044 \nL 487.747515 132.72089 \nL 487.86904 135.765596 \nL 487.895458 135.050296 \nL 487.879607 136.503366 \nL 487.906025 136.001532 \nL 487.985281 141.780106 \nL 488.016983 136.930299 \nL 488.085671 139.922574 \nL 488.096238 134.9342 \nL 488.175494 133.136588 \nL 488.180777 135.087746 \nL 488.281167 138.480739 \nL 488.291735 138.083766 \nL 488.402692 132.604794 \nL 488.413259 133.46615 \nL 488.423827 132.990532 \nL 488.455529 134.612128 \nL 509.299675 119.635772 \nL 509.394782 111.329305 \nL 509.315526 120.253701 \nL 509.431768 113.47146 \nL 509.458186 115.171701 \nL 509.484604 111.894804 \nL 509.542725 113.617516 \nL 509.579711 115.703495 \nL 509.611413 113.4415 \nL 509.616697 111.145799 \nL 509.706519 113.658711 \nL 509.72237 111.497832 \nL 509.812193 115.883256 \nL 509.849179 115.523734 \nL 509.912583 115.890746 \nL 509.96542 113.370344 \nL 510.018257 115.643575 \nL 510.086945 114.943255 \nL 510.097512 115.351463 \nL 510.145065 114.212975 \nL 510.1662 114.898315 \nL 510.192619 112.550184 \nL 510.256023 114.493852 \nL 510.303576 118.377443 \nL 510.36698 116.665967 \nL 510.414533 117.074174 \nL 510.588895 111.681338 \nL 510.641732 113.160623 \nL 510.689285 109.973606 \nL 510.831945 106.408342 \nL 510.858363 105.415911 \nL 510.847796 106.475753 \nL 510.863647 105.846588 \nL 510.958753 111.527792 \nL 510.985172 109.557909 \nL 511.106696 104.741806 \nL 511.117264 105.550731 \nL 511.127831 106.157426 \nL 511.196519 103.18762 \nL 511.21237 104.011526 \nL 511.265207 103.015349 \nL 511.270491 104.66316 \nL 511.281058 104.494634 \nL 516.908185 102.288814 \nL 517.02971 91.622982 \nL 517.034994 91.828959 \nL 517.040277 92.158521 \nL 517.066696 87.645765 \nL 517.071979 88.686882 \nL 517.145951 84.230302 \nL 517.18822 87.039071 \nL 517.204072 84.724646 \nL 517.272759 89.301067 \nL 517.283327 88.518356 \nL 517.288611 88.686882 \nL 517.320313 86.593413 \nL 517.468256 80.893485 \nL 517.341447 86.91174 \nL 517.484107 81.264242 \nL 517.489391 82.155558 \nL 517.536944 72.523353 \nL 517.552795 74.605587 \nL 517.579213 71.643272 \nL 517.626767 75.70288 \nL 517.63205 75.863916 \nL 517.669036 73.822877 \nL 517.690171 71.736898 \nL 517.764142 75.07746 \nL 517.769426 74.676743 \nL 517.77471 75.530608 \nL 517.806412 71.096498 \nL 517.864532 72.246221 \nL 517.880384 72.156341 \nL 517.885667 72.875386 \nL 517.986057 74.384631 \nL 517.938504 71.939129 \nL 517.996625 73.234908 \nL 518.023043 74.695468 \nL 518.049462 72.85666 \nL 518.065313 73.755466 \nL 518.134001 69.11538 \nL 518.17627 71.725663 \nL 518.181554 73.246143 \nL 518.271376 70.321278 \nL 518.281944 70.991638 \nL 518.292511 71.163909 \nL 518.345348 67.430119 \nL 518.47744 60.2172 \nL 518.514426 65.576331 \nL 518.588398 75.234751 \nL 518.635951 72.897856 \nL 518.715206 64.920952 \nL 518.746908 69.583508 \nL 518.868433 82.986954 \nL 518.879 82.582491 \nL 518.889568 80.990855 \nL 524.516695 85.773253 \nL 524.527262 86.511023 \nL 524.543113 79.182008 \nL 524.601234 67.860797 \nL 524.659354 68.508686 \nL 524.733326 78.073481 \nL 524.79673 76.601686 \nL 524.886553 72.699369 \nL 524.912971 75.69539 \nL 524.99751 79.084637 \nL 524.949957 75.575549 \nL 525.034496 78.901131 \nL 525.066198 81.163127 \nL 525.108468 74.938894 \nL 525.119035 75.759055 \nL 525.245844 72.137616 \nL 525.266978 73.373474 \nL 525.272262 74.002638 \nL 525.34095 70.186457 \nL 525.356801 67.407649 \nL 525.425489 71.467256 \nL 525.451907 68.864463 \nL 525.48361 69.347572 \nL 525.504744 67.725976 \nL 525.547014 68.152909 \nL 525.626269 65.119438 \nL 525.584 70.875542 \nL 525.673822 66.212985 \nL 525.679106 66.11936 \nL 525.68439 66.774739 \nL 525.726659 71.99156 \nL 525.795347 66.752269 \nL 525.911588 70.954188 \nL 525.932723 69.725819 \nL 526.027829 65.123183 \nL 526.054248 66.505097 \nL 526.14407 68.471236 \nL 526.096517 66.242946 \nL 526.159922 66.752269 \nL 526.228609 64.100791 \nL 526.255028 68.474981 \nL 526.265595 70.291318 \nL 526.339567 64.973382 \nL 526.350134 66.572508 \nL 526.434673 61.142221 \nL 526.476943 61.209632 \nL 526.492794 63.359276 \nL 526.498077 62.602781 \nL 532.125204 69.673389 \nL 532.167474 61.703975 \nL 532.151623 69.901835 \nL 532.288999 63.2282 \nL 532.336552 66.040714 \nL 532.368254 62.621506 \nL 532.40524 64.445334 \nL 532.547899 68.841993 \nL 532.553183 68.287729 \nL 532.574318 69.05546 \nL 532.611304 66.505097 \nL 532.627155 65.875933 \nL 532.679992 68.257769 \nL 532.711694 67.224142 \nL 532.727545 67.845817 \nL 532.769814 66.21673 \nL 532.780382 65.332905 \nL 532.875488 67.070596 \nL 532.886055 66.961991 \nL 533.018148 75.126146 \nL 533.092119 79.927268 \nL 533.139672 76.009972 \nL 533.144956 75.358337 \nL 533.192509 80.455317 \nL 533.229495 78.549099 \nL 533.314034 86.537238 \nL 533.356304 82.754762 \nL 533.361587 81.893406 \nL 533.424992 87.630785 \nL 533.45141 84.889427 \nL 533.461977 84.110461 \nL 533.467261 82.245439 \nL 533.541233 85.256439 \nL 533.567651 84.69843 \nL 533.609921 95.731275 \nL 533.620488 95.165776 \nL 533.65219 102.139013 \nL 533.736729 99.120523 \nL 533.826552 109.857511 \nL 533.868821 104.53583 \nL 533.900523 100.277736 \nL 533.926942 105.513281 \nL 533.979779 102.891763 \nL 533.990346 103.502203 \nL 534.006197 99.742197 \nL 534.011481 95.562749 \nL 534.064318 101.861881 \nL 534.106587 99.184188 \nL 539.733714 117.542302 \nL 539.91336 129.653714 \nL 539.934494 127.069647 \nL 540.108856 111.422931 \nL 540.124707 111.456636 \nL 540.129991 111.452891 \nL 540.151125 113.580066 \nL 540.188111 105.022683 \nL 540.203962 106.02635 \nL 540.209246 104.734316 \nL 540.283218 112.902216 \nL 540.336055 112.561419 \nL 540.410026 118.504774 \nL 540.41531 118.572184 \nL 540.420594 117.590988 \nL 540.526267 112.827316 \nL 540.457579 118.950432 \nL 540.557969 113.647476 \nL 540.563253 113.662456 \nL 540.568537 113.467715 \nL 540.579104 110.49042 \nL 540.642508 115.328993 \nL 540.679494 113.228033 \nL 540.742898 118.141507 \nL 540.779884 112.69624 \nL 540.785168 112.947156 \nL 540.838005 114.036959 \nL 540.801019 111.95098 \nL 540.848572 113.205563 \nL 540.890842 106.408342 \nL 540.943679 114.212975 \nL 540.95953 113.235523 \nL 540.975381 112.415363 \nL 541.033501 116.006842 \nL 541.038785 115.643575 \nL 541.086338 114.553772 \nL 541.16031 117.87561 \nL 541.165593 117.86063 \nL 541.2607 112.246837 \nL 541.31882 113.5164 \nL 541.403359 123.227251 \nL 541.350522 113.497675 \nL 541.445629 120.122625 \nL 541.487898 110.55783 \nL 541.556586 116.561106 \nL 541.66226 112.553929 \nL 541.598856 117.253936 \nL 541.678111 113.53138 \nL 541.70453 115.216642 \nL 541.709813 113.359109 \nL 541.715097 114.98445 \nL 562.559243 97.779804 \nL 562.564527 95.806176 \nL 562.664917 99.577416 \nL 562.670201 101.446184 \nL 562.744172 94.93733 \nL 562.765307 95.974702 \nL 562.881548 91.357085 \nL 562.802293 96.693747 \nL 562.886832 91.97876 \nL 562.981938 91.29342 \nL 563.003073 93.862507 \nL 563.01364 92.915016 \nL 563.045343 94.881154 \nL 563.061194 94.105934 \nL 563.161584 97.862194 \nL 563.124598 93.308244 \nL 563.177435 96.131993 \nL 563.198569 97.727374 \nL 563.230272 95.656375 \nL 563.272541 96.880998 \nL 563.330662 94.843704 \nL 563.388782 94.847449 \nL 563.39935 95.461633 \nL 563.468037 93.828802 \nL 563.494456 92.192226 \nL 563.563144 96.060837 \nL 563.573711 94.25199 \nL 563.578995 94.375576 \nL 563.589562 92.750235 \nL 563.60013 93.162188 \nL 563.642399 91.518122 \nL 563.705803 93.233343 \nL 563.711087 92.971191 \nL 563.742789 91.589277 \nL 563.763924 94.345616 \nL 563.790342 92.735255 \nL 563.827328 112.853531 \nL 563.922435 103.434792 \nL 563.980555 103.09774 \nL 564.054527 108.700298 \nL 564.096796 105.292325 \nL 564.170768 106.400852 \nL 564.297576 114.785964 \nL 564.313427 112.894726 \nL 564.40325 103.457262 \nL 564.450803 106.269776 \nL 564.508924 104.61073 \nL 564.482505 106.603084 \nL 564.540626 105.633122 \nL 570.167753 92.858841 \nL 570.215306 88.492141 \nL 570.27871 90.664256 \nL 570.310413 92.802665 \nL 570.357966 89.634374 \nL 570.3791 88.896604 \nL 570.405519 89.143775 \nL 570.410803 88.39477 \nL 570.442505 92.222186 \nL 570.505909 90.267283 \nL 570.511193 90.323458 \nL 570.542895 89.529513 \nL 570.569313 88.012778 \nL 570.606299 89.709274 \nL 570.674987 92.368242 \nL 570.717256 90.042582 \nL 570.82293 86.140265 \nL 570.785944 90.207363 \nL 570.838781 86.885525 \nL 570.849349 87.851742 \nL 570.92332 84.65349 \nL 570.933888 84.915642 \nL 570.939171 85.039228 \nL 570.949739 84.619785 \nL 571.06598 76.826387 \nL 571.071264 76.875073 \nL 571.081831 78.174596 \nL 571.118817 72.800485 \nL 571.171654 76.44814 \nL 571.213923 72.11889 \nL 571.256193 76.976188 \nL 571.335448 74.163674 \nL 571.346015 74.3322 \nL 571.356583 73.294828 \nL 571.361866 73.437139 \nL 571.393568 70.115302 \nL 571.472824 72.605744 \nL 571.504526 73.905267 \nL 571.557363 68.894424 \nL 571.562646 69.07793 \nL 571.673604 66.516333 \nL 571.599632 71.148929 \nL 571.678888 67.123027 \nL 571.721157 71.482236 \nL 571.763427 66.913305 \nL 571.795129 68.175379 \nL 571.800412 67.950677 \nL 571.821547 70.246378 \nL 571.853249 69.546058 \nL 571.863817 70.710761 \nL 571.927221 68.463746 \nL 571.958923 68.838248 \nL 572.006476 70.845582 \nL 571.995909 68.033068 \nL 572.075164 69.729564 \nL 572.085731 69.527333 \nL 572.106866 70.766936 \nL 572.143852 72.351082 \nL 572.128001 70.64335 \nL 572.149136 72.302397 \nL 577.776263 79.425435 \nL 577.781546 78.755075 \nL 577.839667 83.421377 \nL 577.866085 86.994131 \nL 577.918922 80.537707 \nL 577.945341 82.166793 \nL 578.003461 85.174049 \nL 578.056298 82.912053 \nL 578.320482 68.197849 \nL 578.109135 84.62353 \nL 578.33105 68.692192 \nL 578.341617 71.441041 \nL 578.399738 67.257848 \nL 578.442007 69.209006 \nL 578.568816 71.560882 \nL 578.484277 68.890679 \nL 578.579383 70.268848 \nL 578.653355 68.699682 \nL 578.67449 71.500961 \nL 578.679773 73.028932 \nL 578.753745 68.639762 \nL 578.780163 70.119047 \nL 578.833 71.894189 \nL 578.822433 67.56494 \nL 578.864702 68.235299 \nL 578.87527 68.328925 \nL 578.880553 67.954422 \nL 579.039064 60.550507 \nL 579.044348 62.134653 \nL 579.165872 67.475059 \nL 579.171156 67.145497 \nL 579.208142 70.302553 \nL 579.250411 66.437687 \nL 579.260979 66.497607 \nL 579.266262 65.879678 \nL 579.303248 68.227809 \nL 579.361369 67.628605 \nL 579.440624 70.905502 \nL 579.403638 67.119282 \nL 579.493461 68.999284 \nL 579.498745 69.014264 \nL 579.519879 69.77076 \nL 579.541014 66.853385 \nL 579.588567 67.827091 \nL 579.593851 67.812111 \nL 579.725943 74.380886 \nL 579.62027 67.475059 \nL 579.741794 72.680644 \nL 579.747078 72.77427 \nL 579.752362 72.100165 \nL 579.757645 70.531 \nL 585.384772 69.958011 \nL 585.390056 70.781916 \nL 585.45346 64.059596 \nL 585.464028 65.482706 \nL 585.469311 65.47147 \nL 585.569701 60.531782 \nL 585.590836 62.572821 \nL 585.59612 64.954657 \nL 585.617255 61.70023 \nL 585.69651 62.580311 \nL 585.733496 61.243337 \nL 585.722928 62.917363 \nL 585.807467 62.47545 \nL 585.818035 62.078478 \nL 585.823318 62.40804 \nL 585.860304 50.364038 \nL 585.944843 51.495036 \nL 585.950127 51.614877 \nL 585.971262 49.997026 \nL 585.981829 50.408979 \nL 586.076935 46.956065 \nL 586.103354 48.723717 \nL 586.224879 52.176631 \nL 586.135056 48.615111 \nL 586.230162 52.150415 \nL 586.314701 48.409135 \nL 586.351687 49.997026 \nL 586.425659 47.173277 \nL 586.473212 48.989614 \nL 586.573602 58.281022 \nL 586.631723 53.798227 \nL 586.668708 52.09424 \nL 586.705694 56.854167 \nL 586.721545 56.180063 \nL 586.790233 52.655994 \nL 586.753247 57.187474 \nL 586.837786 53.513605 \nL 586.858921 49.08324 \nL 586.948744 51.524996 \nL 587.022715 49.89591 \nL 587.001581 52.592328 \nL 587.064985 51.187944 \nL 587.191793 57.191219 \nL 587.202361 55.951616 \nL 587.24463 50.899577 \nL 587.318602 52.715914 \nL 587.350304 51.37145 \nL 587.366155 53.345078 \nL 592.993282 52.292726 \nL 593.030268 59.030027 \nL 593.06197 51.528741 \nL 593.109523 56.138867 \nL 593.130658 53.813207 \nL 593.172927 57.112574 \nL 593.209913 56.273688 \nL 593.342005 64.947167 \nL 593.431828 58.337197 \nL 593.511083 59.325884 \nL 593.526935 61.808836 \nL 593.585055 57.97393 \nL 593.611474 58.221101 \nL 593.648459 56.49839 \nL 593.70658 59.722857 \nL 593.759417 63.778719 \nL 593.822821 62.187083 \nL 593.849239 61.415608 \nL 593.886225 65.302944 \nL 593.991899 68.239044 \nL 593.923211 63.333061 \nL 593.997183 67.718486 \nL 594.092289 63.98095 \nL 594.113424 64.535214 \nL 594.192679 67.205417 \nL 594.219098 65.512666 \nL 594.240232 61.486764 \nL 594.330055 64.419118 \nL 594.340622 63.793699 \nL 594.388176 66.445177 \nL 594.40931 64.924697 \nL 594.499133 69.340082 \nL 594.620658 61.939912 \nL 594.631225 61.520469 \nL 594.641793 62.898638 \nL 594.726332 61.011145 \nL 594.75275 63.864855 \nL 594.768601 59.692896 \nL 594.832005 64.231867 \nL 594.863707 62.939833 \nL 594.884842 62.741347 \nL 594.900693 64.834816 \nL 594.916544 65.587566 \nL 594.937679 63.471627 \nL 594.974665 64.044616 \nL 615.818811 103.974076 \nL 615.824095 109.943646 \nL 615.892783 100.749609 \nL 615.924485 102.528496 \nL 615.929769 103.569613 \nL 615.966754 97.008329 \nL 616.00374 97.498927 \nL 616.077712 94.510397 \nL 616.061861 97.674943 \nL 616.114698 96.566416 \nL 616.167535 99.45383 \nL 616.125265 96.278049 \nL 616.24679 99.202913 \nL 616.30491 96.944663 \nL 616.352464 99.090563 \nL 616.373598 98.761001 \nL 616.505691 103.584593 \nL 616.574378 101.562279 \nL 616.537393 104.723081 \nL 616.606081 103.588338 \nL 616.701187 107.932568 \nL 616.727605 106.74914 \nL 616.732889 105.674317 \nL 616.785726 108.29958 \nL 616.822712 107.981253 \nL 616.99179 124.133547 \nL 617.002357 123.186056 \nL 617.134449 119.182624 \nL 617.055194 126.78877 \nL 617.145017 119.44103 \nL 617.208421 116.048037 \nL 617.160868 119.736887 \nL 617.282393 116.40756 \nL 617.329946 113.662456 \nL 617.403917 119.437285 \nL 617.430336 120.93904 \nL 617.451471 118.407403 \nL 617.456754 117.11537 \nL 617.541293 122.980079 \nL 617.557144 123.976256 \nL 617.567712 119.40358 \nL 617.583563 119.808043 \nL 617.710371 113.42652 \nL 617.720939 113.100702 \nL 617.731506 114.066919 \nL 617.763208 111.804924 \nL 617.800194 113.216798 \nL 623.427321 132.994277 \nL 623.532995 153.38594 \nL 623.554129 151.535897 \nL 623.580548 147.783382 \nL 623.675654 148.360116 \nL 623.723207 144.858517 \nL 623.75491 150.217648 \nL 623.781328 148.004338 \nL 623.902853 161.688661 \nL 623.939839 159.891049 \nL 623.960973 156.621642 \nL 624.003243 160.999576 \nL 624.008527 160.939656 \nL 624.124768 163.950656 \nL 624.087782 159.426666 \nL 624.130051 163.830815 \nL 624.15647 159.097103 \nL 624.251576 160.445313 \nL 624.267427 159.516546 \nL 624.320264 164.767072 \nL 624.346682 167.972813 \nL 624.425938 163.636074 \nL 624.526328 158.164592 \nL 624.563314 159.954714 \nL 624.568597 161.059497 \nL 624.637285 154.726659 \nL 624.653136 155.883872 \nL 624.674271 158.329373 \nL 624.695406 152.801716 \nL 624.748243 153.824108 \nL 624.774661 158.115907 \nL 624.790512 153.535741 \nL 624.848633 153.876538 \nL 624.922604 152.876616 \nL 624.949023 155.752796 \nL 624.986009 154.584348 \nL 625.028278 158.951047 \nL 625.049413 163.69974 \nL 625.144519 162.815914 \nL 625.20264 160.834795 \nL 625.176221 163.504998 \nL 625.250193 162.104359 \nL 625.345299 166.530979 \nL 625.36115 164.295199 \nL 625.371718 162.97695 \nL 625.398136 164.5199 \nL 625.408704 163.845796 \nL 638.64434 141.62656 \nL 638.765865 128.013393 \nL 638.876823 126.032275 \nL 638.818702 130.016982 \nL 638.882106 126.889886 \nL 638.993064 132.702165 \nL 638.913808 126.290682 \nL 639.008915 130.900808 \nL 639.162142 121.67681 \nL 639.051184 131.016904 \nL 639.220262 123.650439 \nL 639.33122 125.605342 \nL 639.273099 122.729163 \nL 639.336503 125.316975 \nL 639.341787 125.309485 \nL 639.405191 128.567657 \nL 639.458028 126.511638 \nL 639.468596 123.957531 \nL 639.526716 127.309329 \nL 639.568986 126.062235 \nL 639.574269 126.069725 \nL 639.664092 131.829574 \nL 639.701078 129.608774 \nL 639.854305 137.465837 \nL 639.859588 137.166235 \nL 639.949411 132.436268 \nL 639.912425 138.465759 \nL 639.996964 132.896906 \nL 640.018099 132.889416 \nL 640.23473 139.282174 \nL 640.277 151.374861 \nL 640.361539 147.944418 \nL 640.366822 147.910713 \nL 640.372106 148.049279 \nL 640.414376 152.66315 \nL 640.467212 146.727285 \nL 640.47778 147.322744 \nL 640.594021 142.36433 \nL 640.609872 142.678912 \nL 640.615156 143.663854 \nL 640.620439 142.431741 \nL 640.625723 142.431741 \nL 646.25285 142.469191 \nL 646.305687 134.162725 \nL 646.337389 154.940125 \nL 646.411361 163.388902 \nL 646.443063 149.266412 \nL 646.575155 129.507658 \nL 646.59629 132.406308 \nL 646.606857 131.96814 \nL 646.7548 147.300274 \nL 646.760084 146.528798 \nL 646.85519 136.907828 \nL 646.770651 146.933261 \nL 646.923878 138.67548 \nL 646.929162 138.368388 \nL 646.976715 140.918751 \nL 647.003134 140.484328 \nL 647.013701 141.637795 \nL 647.071822 137.312291 \nL 647.09824 133.409975 \nL 647.188063 135.862966 \nL 647.193346 137.866555 \nL 647.246183 133.754517 \nL 647.293736 136.207509 \nL 647.446963 127.620166 \nL 647.452247 128.410366 \nL 647.4998 124.459364 \nL 647.536786 128.837299 \nL 647.584339 126.616499 \nL 647.605474 129.305427 \nL 647.658311 122.946374 \nL 647.679446 125.017373 \nL 647.763985 121.852827 \nL 647.753417 125.178409 \nL 647.795687 122.968844 \nL 647.896077 127.260643 \nL 647.911928 124.60542 \nL 648.038736 114.20174 \nL 647.922495 124.897532 \nL 648.070438 115.69975 \nL 648.18668 120.88661 \nL 648.107424 114.190505 \nL 648.191963 120.508363 \nL 648.197247 121.452109 \nL 648.228949 119.819278 \nL 648.234233 119.819278 \nL 669.083663 135.638265 \nL 669.141783 119.235054 \nL 669.104798 139.43572 \nL 669.215755 127.796182 \nL 669.321429 124.347013 \nL 669.247457 132.612284 \nL 669.33728 125.732673 \nL 669.427102 128.189409 \nL 669.384833 124.335778 \nL 669.448237 126.298172 \nL 669.511641 112.973372 \nL 669.590897 115.954412 \nL 669.59618 117.029234 \nL 669.670152 110.56532 \nL 669.675436 110.696396 \nL 669.892067 124.972433 \nL 669.944904 124.905022 \nL 669.98189 121.826611 \nL 670.055861 123.721594 \nL 670.156251 117.752024 \nL 670.177386 118.751946 \nL 670.187953 122.182389 \nL 670.277776 117.295131 \nL 670.28306 117.358796 \nL 670.335897 121.459599 \nL 670.394017 119.452265 \nL 670.504975 115.396403 \nL 670.44157 121.309798 \nL 670.526109 116.171623 \nL 670.600081 121.088841 \nL 670.536677 115.403893 \nL 670.658202 118.373698 \nL 670.68462 120.062705 \nL 670.72689 117.710829 \nL 670.732173 116.669712 \nL 670.774443 124.380719 \nL 670.811429 121.938962 \nL 670.922386 124.571715 \nL 670.869549 121.497049 \nL 670.92767 124.474344 \nL 670.969939 122.976334 \nL 671.001641 124.773946 \nL 671.02806 124.388209 \nL 671.054478 126.208291 \nL 671.059762 125.639047 \nL 676.686889 113.767317 \nL 676.713307 108.250895 \nL 676.750293 114.089389 \nL 676.80313 111.329305 \nL 676.829548 118.201427 \nL 676.908804 111.906039 \nL 676.929938 108.640377 \nL 676.998626 112.130741 \nL 677.030328 109.123486 \nL 677.035612 109.602849 \nL 677.109584 106.805315 \nL 677.120151 107.831452 \nL 677.125435 107.206033 \nL 677.167704 112.273052 \nL 677.20469 110.909863 \nL 677.437172 117.377521 \nL 677.220541 110.56532 \nL 677.453023 116.613536 \nL 677.521711 111.797434 \nL 677.463591 116.770827 \nL 677.595683 113.149388 \nL 677.780612 128.904709 \nL 677.791179 127.680086 \nL 677.812314 124.223428 \nL 677.881002 132.66097 \nL 677.886286 131.323996 \nL 677.89157 131.147979 \nL 677.902137 134.087824 \nL 677.939123 140.948711 \nL 678.002527 132.099216 \nL 678.102917 124.51554 \nL 678.081782 132.192842 \nL 678.118768 128.365426 \nL 678.266711 134.612128 \nL 678.324832 133.38376 \nL 678.335399 136.278664 \nL 678.367101 134.694518 \nL 678.382952 136.911573 \nL 678.430506 132.496189 \nL 678.483343 136.132608 \nL 678.504477 138.027591 \nL 678.525612 134.402406 \nL 678.589016 136.020257 \nL 678.626002 134.046629 \nL 678.610151 136.641932 \nL 678.668272 135.544639 \nL 684.295399 144.146962 \nL 684.422207 154.09375 \nL 684.438058 155.280923 \nL 684.480328 156.587936 \nL 684.527881 153.157493 \nL 684.533164 151.472232 \nL 684.622987 156.992399 \nL 684.628271 156.760208 \nL 684.654689 159.932244 \nL 684.77093 182.342476 \nL 684.839618 179.773388 \nL 684.987562 168.93154 \nL 684.866037 181.597216 \nL 685.003413 169.276082 \nL 685.140788 177.384062 \nL 685.294015 169.789151 \nL 685.325718 172.073616 \nL 685.399689 174.451707 \nL 685.394405 170.339669 \nL 685.431391 171.084929 \nL 685.441959 171.339591 \nL 685.447242 170.882698 \nL 685.552916 169.002695 \nL 685.478944 174.036009 \nL 685.563483 169.493294 \nL 685.568767 169.867796 \nL 685.605753 164.879423 \nL 685.753696 152.494624 \nL 685.61632 165.025478 \nL 685.75898 153.172473 \nL 685.848803 160.894716 \nL 685.875221 157.12722 \nL 685.885788 155.887617 \nL 685.912207 161.295433 \nL 685.928058 168.624448 \nL 686.028448 167.823012 \nL 686.171108 153.861558 \nL 686.181675 155.554309 \nL 686.186959 155.895107 \nL 686.223944 152.753031 \nL 686.266214 155.086181 \nL 686.276781 153.902753 \nL 691.903908 144.465289 \nL 691.930327 138.885202 \nL 691.993731 149.288882 \nL 691.999015 148.28896 \nL 692.020149 151.97032 \nL 692.083554 144.551425 \nL 692.099405 145.075729 \nL 692.183944 142.162099 \nL 692.131107 147.015652 \nL 692.210362 144.293018 \nL 692.27905 151.629523 \nL 692.226213 143.873575 \nL 692.331887 145.671188 \nL 692.432277 142.053493 \nL 692.347738 146.240431 \nL 692.448128 144.034612 \nL 692.469263 142.828713 \nL 692.500965 146.01573 \nL 692.511532 146.427683 \nL 692.537951 144.648796 \nL 692.601355 148.427526 \nL 692.611922 148.053024 \nL 692.72288 151.741874 \nL 692.68061 146.540034 \nL 692.733447 150.584661 \nL 692.775717 147.202903 \nL 692.828554 151.386096 \nL 692.913093 157.763874 \nL 692.928944 157.314471 \nL 693.013483 149.416213 \nL 693.039901 151.622033 \nL 693.045185 152.299882 \nL 693.108589 146.592464 \nL 693.282951 137.836595 \nL 693.156142 148.891909 \nL 693.288234 138.199862 \nL 693.356922 144.491505 \nL 693.415043 141.139707 \nL 693.42561 139.971259 \nL 693.478447 145.068239 \nL 693.489014 144.652541 \nL 693.504865 147.086807 \nL 693.562986 142.491661 \nL 693.584121 143.933496 \nL 693.589404 141.409349 \nL 693.684511 146.154296 \nL 693.689795 144.914693 \nL 693.72678 141.132217 \nL 693.758482 145.173099 \nL 693.790185 144.903457 \nL 693.795468 145.749833 \nL 693.86944 140.356997 \nL 693.880007 139.035003 \nL 693.885291 143.480348 \nL 699.512418 149.933026 \nL 699.538836 143.038435 \nL 699.565255 153.981399 \nL 699.623375 146.551269 \nL 699.68678 152.303627 \nL 699.739616 149.536054 \nL 699.7449 147.367684 \nL 699.834723 156.018693 \nL 699.898127 158.130887 \nL 699.935113 154.827775 \nL 700.051354 152.719325 \nL 700.019652 156.756463 \nL 700.056638 152.790481 \nL 700.141177 154.603073 \nL 700.172879 153.539486 \nL 700.194014 152.539564 \nL 700.215148 155.265943 \nL 700.225716 156.572956 \nL 700.299687 152.075181 \nL 700.304971 152.68562 \nL 700.394794 150.543466 \nL 700.363092 152.824186 \nL 700.400077 151.685698 \nL 700.4899 153.46833 \nL 700.442347 151.161395 \nL 700.511035 151.91789 \nL 700.526886 152.168807 \nL 700.521602 151.854224 \nL 700.53217 151.899165 \nL 700.537453 152.681875 \nL 700.616709 148.26649 \nL 700.621992 147.824577 \nL 700.69068 150.865538 \nL 700.701248 150.049122 \nL 700.73295 151.386096 \nL 700.817489 151.21757 \nL 701.012985 140.401937 \nL 700.896744 152.809206 \nL 701.018269 141.461779 \nL 701.166212 145.835969 \nL 701.171496 144.51023 \nL 701.182063 147.779637 \nL 701.266602 147.023142 \nL 701.33529 154.599328 \nL 701.382843 153.344745 \nL 701.462099 148.783304 \nL 701.493801 151.505937 \nL 722.337947 138.581855 \nL 722.343231 139.270939 \nL 722.359082 134.394916 \nL 722.422486 135.436033 \nL 722.496458 139.091178 \nL 722.464756 133.642166 \nL 722.538727 136.181293 \nL 722.623266 134.424877 \nL 722.596848 137.821615 \nL 722.633834 137.14751 \nL 722.639117 137.990141 \nL 722.654968 135.166392 \nL 722.744791 137.791654 \nL 722.998408 130.792202 \nL 722.765926 137.88528 \nL 723.014259 132.181606 \nL 723.141068 137.889025 \nL 723.151635 136.529581 \nL 723.236174 135.870456 \nL 723.204472 138.754126 \nL 723.257309 137.076355 \nL 723.310146 137.701774 \nL 723.384117 135.113961 \nL 723.505642 136.641932 \nL 723.421103 133.915553 \nL 723.510926 136.151333 \nL 723.653585 133.728302 \nL 723.542628 136.274919 \nL 723.658869 134.432367 \nL 723.690571 137.357231 \nL 723.764543 135.956592 \nL 723.77511 134.012924 \nL 723.843798 138.559385 \nL 723.870216 136.982729 \nL 723.896635 135.892927 \nL 723.954755 138.799066 \nL 723.965323 138.447034 \nL 724.081564 149.536054 \nL 724.11855 146.892066 \nL 724.21894 140.634129 \nL 724.266493 142.087198 \nL 724.271777 142.723853 \nL 724.303479 140.364487 \nL 724.31933 140.922496 \nL 729.946457 137.21492 \nL 729.972875 137.091335 \nL 730.067982 143.645129 \nL 730.242343 133.024237 \nL 730.258194 133.117863 \nL 730.369152 136.327349 \nL 730.374435 136.323604 \nL 730.469542 132.114196 \nL 730.49596 133.439935 \nL 730.543513 136.304879 \nL 730.612201 135.537149 \nL 730.654471 131.788379 \nL 730.754861 133.630931 \nL 730.807698 136.398505 \nL 730.791847 131.994355 \nL 730.860535 133.226469 \nL 730.865818 133.44368 \nL 730.89752 132.571089 \nL 730.913372 132.851966 \nL 730.934506 127.313074 \nL 731.029613 128.990845 \nL 731.050747 129.833476 \nL 731.071882 128.215625 \nL 731.130003 128.260565 \nL 731.214542 122.755378 \nL 731.28323 126.06598 \nL 731.293797 127.623911 \nL 731.336067 124.332033 \nL 731.394187 126.339367 \nL 731.478726 122.478246 \nL 731.410038 126.934826 \nL 731.505145 125.114744 \nL 731.526279 126.575304 \nL 731.579116 120.80422 \nL 731.695357 104.547065 \nL 731.727059 108.689063 \nL 731.74291 107.752806 \nL 731.795747 103.079015 \nL 731.875003 104.696866 \nL 731.880286 106.075035 \nL 731.906705 104.389774 \nL 731.927839 105.517026 \nL 737.554966 89.81788 \nL 737.676491 77.384396 \nL 737.565534 91.769038 \nL 737.692342 78.155871 \nL 737.734612 79.43667 \nL 737.708193 75.860171 \nL 737.750463 76.571725 \nL 737.813867 70.856817 \nL 737.877271 71.515941 \nL 737.882555 72.276181 \nL 737.940676 64.771151 \nL 738.16259 47.266902 \nL 738.183725 47.439174 \nL 738.236562 59.831462 \nL 738.321101 59.483175 \nL 738.326385 58.955126 \nL 738.34752 64.426608 \nL 738.352803 68.523666 \nL 738.44791 59.955048 \nL 738.527165 50.229217 \nL 738.564151 55.408587 \nL 738.664541 48.42786 \nL 738.595853 55.786835 \nL 738.717378 50.948262 \nL 738.786066 57.123809 \nL 738.828335 52.10922 \nL 738.860037 50.277903 \nL 738.84947 53.989223 \nL 738.897023 53.468664 \nL 738.955144 54.521016 \nL 738.965711 48.057103 \nL 739.092519 36.252783 \nL 739.118938 40.361076 \nL 739.145356 32.290546 \nL 739.155924 32.84481 \nL 739.198193 29.414367 \nL 739.245746 37.436211 \nL 739.25103 34.736048 \nL 739.282732 43.424506 \nL 739.372555 39.45478 \nL 739.39369 38.466093 \nL 739.462378 42.41335 \nL 739.478229 49.14316 \nL 739.536349 46.858695 \nL 745.163476 44.817656 \nL 745.263866 64.149477 \nL 745.279717 51.978144 \nL 745.285001 51.955674 \nL 745.337838 49.334156 \nL 745.406526 59.614251 \nL 745.422377 57.726758 \nL 745.46993 63.033459 \nL 745.5122 59.655446 \nL 745.61259 69.943031 \nL 745.675994 67.231632 \nL 745.734114 62.220788 \nL 745.786951 65.583821 \nL 745.903192 67.254103 \nL 745.808086 64.127006 \nL 745.908476 66.295376 \nL 746.019433 56.644446 \nL 746.061703 61.142221 \nL 746.119824 54.022928 \nL 746.220214 57.086359 \nL 746.236065 59.764052 \nL 746.299469 47.903557 \nL 746.331171 42.821557 \nL 746.368157 50.712326 \nL 746.410426 46.810009 \nL 746.468547 49.382842 \nL 746.426277 46.798774 \nL 746.5161 47.169532 \nL 746.531951 44.810166 \nL 746.600639 52.768345 \nL 746.605923 54.165239 \nL 746.690462 47.544034 \nL 746.695745 47.933517 \nL 746.727448 42.499485 \nL 746.743299 43.802754 \nL 746.753866 40.731833 \nL 746.81727 49.285471 \nL 746.822554 48.39041 \nL 746.864823 52.622289 \nL 746.901809 48.139493 \nL 746.933511 50.173042 \nL 747.134292 62.954813 \nL 747.007483 48.607621 \nL 747.144859 60.823894 \nL 752.771986 58.72668 \nL 752.824823 42.945143 \nL 752.888227 48.083318 \nL 752.956915 44.978692 \nL 752.919929 49.139415 \nL 752.97805 48.386665 \nL 752.988617 50.386508 \nL 753.030887 40.020278 \nL 753.083723 46.87742 \nL 753.094291 43.754069 \nL 753.173546 49.91838 \nL 753.284504 63.179515 \nL 753.295071 63.029714 \nL 753.427163 54.31504 \nL 753.432447 53.989223 \nL 753.490567 56.884127 \nL 753.553972 62.007322 \nL 753.590957 56.299903 \nL 753.596241 54.99289 \nL 753.627943 58.030105 \nL 753.691347 57.400941 \nL 753.701915 58.951381 \nL 753.775886 56.191298 \nL 753.78117 52.929381 \nL 753.802305 58.082535 \nL 753.88156 55.535918 \nL 753.955532 62.801267 \nL 754.013652 61.76015 \nL 754.108759 56.783012 \nL 754.071773 65.213064 \nL 754.135177 58.68923 \nL 754.240851 61.831306 \nL 754.151028 58.625564 \nL 754.261986 61.336962 \nL 754.314823 57.970185 \nL 754.272553 62.254494 \nL 754.388794 58.367157 \nL 754.510319 64.531469 \nL 754.552588 61.894971 \nL 754.56844 65.081988 \nL 754.62656 63.179515 \nL 754.637127 64.040871 \nL 754.66883 61.348198 \nL 754.711099 52.951851 \nL 754.753369 55.367392 \nL 775.597515 54.831854 \nL 775.740175 38.406172 \nL 775.671487 57.599427 \nL 775.756026 38.878046 \nL 775.782444 42.926418 \nL 775.808862 33.601305 \nL 775.851132 36.829517 \nL 775.861699 35.930711 \nL 775.898685 40.432231 \nL 775.925104 40.28243 \nL 776.041345 45.825068 \nL 775.972657 36.664736 \nL 776.046628 44.330803 \nL 776.067763 43.229765 \nL 776.099465 48.521486 \nL 776.136451 46.720129 \nL 776.194572 48.761167 \nL 776.210423 46.016064 \nL 776.284394 40.701873 \nL 776.326664 43.360841 \nL 776.36365 41.083866 \nL 776.395352 46.278216 \nL 776.506309 48.982124 \nL 776.411203 45.162198 \nL 776.52216 47.832401 \nL 776.527444 46.278216 \nL 776.590848 52.07177 \nL 776.633118 46.821244 \nL 776.701806 45.993594 \nL 776.759926 51.438861 \nL 776.839181 54.940459 \nL 776.807479 49.794794 \nL 776.855033 52.337667 \nL 776.902586 47.544034 \nL 776.960706 54.041653 \nL 776.96599 53.989223 \nL 777.024111 62.247004 \nL 777.087515 56.333609 \nL 777.092798 57.093849 \nL 777.135068 52.127945 \nL 777.16677 52.390097 \nL 777.261876 44.233432 \nL 777.198472 52.641014 \nL 777.288295 46.042279 \nL 777.30943 43.99375 \nL 777.341132 51.723483 \nL 777.36755 49.461487 \nL 777.404536 51.682287 \nL 777.441522 44.637895 \nL 777.457373 44.780206 \nL 777.541912 41.978927 \nL 777.56833 44.71654 \nL 777.578898 44.600444 \nL 783.206025 37.593502 \nL 783.274713 26.695478 \nL 783.221876 37.855654 \nL 783.327549 29.084804 \nL 783.375103 33.305448 \nL 783.396237 25.751732 \nL 783.406805 22.778182 \nL 783.48606 27.70289 \nL 783.491344 27.448228 \nL 783.602301 32.571423 \nL 783.517762 25.100098 \nL 783.618152 31.226959 \nL 783.744961 24.927826 \nL 783.681557 32.788634 \nL 783.755528 26.201135 \nL 783.834783 31.215724 \nL 783.792514 25.624401 \nL 783.882337 28.736517 \nL 783.945741 27.508149 \nL 783.914039 29.13349 \nL 783.998578 27.661695 \nL 784.003861 28.624166 \nL 784.072549 24.70687 \nL 784.093684 25.42217 \nL 784.278613 28.624166 \nL 784.283897 28.152293 \nL 784.400138 32.695009 \nL 784.415989 27.006315 \nL 784.521663 30.163372 \nL 784.526946 31.672617 \nL 784.595634 27.68042 \nL 784.63262 30.69891 \nL 784.637904 29.826319 \nL 784.680173 39.241313 \nL 784.701308 37.114139 \nL 784.785847 51.569936 \nL 784.828117 46.180845 \nL 784.912656 42.016377 \nL 784.859819 48.349215 \nL 784.939074 43.952555 \nL 784.944358 45.229609 \nL 785.03418 42.050082 \nL 785.081734 40.664423 \nL 785.118719 44.405703 \nL 785.134571 43.379566 \nL 785.166273 41.956456 \nL 785.17684 44.076141 \nL 785.187407 44.907536 \nL 790.814534 36.043061 \nL 790.899073 32.530228 \nL 790.85152 39.436054 \nL 790.920208 37.222744 \nL 791.099853 63.864855 \nL 791.110421 63.067164 \nL 791.163258 57.723013 \nL 791.205527 64.449079 \nL 791.321768 77.904955 \nL 791.327052 75.890131 \nL 791.411591 70.980403 \nL 791.438009 74.335945 \nL 791.474995 79.47412 \nL 791.527832 71.740643 \nL 791.543683 74.238575 \nL 791.564818 71.661997 \nL 791.633506 75.448218 \nL 791.649357 73.321044 \nL 791.659924 77.29077 \nL 791.755031 72.433473 \nL 791.760314 72.916581 \nL 791.7973 66.310356 \nL 791.802584 67.755936 \nL 791.89769 63.231945 \nL 791.913541 67.291553 \nL 791.93996 69.10789 \nL 792.003364 64.280552 \nL 792.019215 66.411472 \nL 792.029782 66.823425 \nL 792.061485 64.943422 \nL 792.093187 66.15681 \nL 792.24113 54.603407 \nL 792.352087 60.752739 \nL 792.288683 53.127867 \nL 792.367938 58.558154 \nL 792.373222 57.423411 \nL 792.44191 59.891383 \nL 792.463045 59.45696 \nL 792.579286 64.318003 \nL 792.679676 58.251062 \nL 792.589853 65.980794 \nL 792.74308 59.831462 \nL 792.790633 61.924931 \nL 792.753648 59.119908 \nL 792.795917 61.456803 \nL 798.423044 50.895832 \nL 798.528718 66.770994 \nL 798.438895 48.836068 \nL 798.576271 65.928364 \nL 798.666094 53.659661 \nL 798.671377 50.577505 \nL 798.7612 57.161259 \nL 798.771767 55.876716 \nL 798.86159 52.558623 \nL 798.877441 54.99289 \nL 798.898576 57.393451 \nL 798.967264 52.734639 \nL 798.988399 55.0603 \nL 799.030668 53.940537 \nL 799.06237 56.213768 \nL 799.094072 55.273767 \nL 799.168044 61.389393 \nL 799.215597 58.247317 \nL 799.342406 55.11273 \nL 799.426945 59.48692 \nL 799.448079 56.007791 \nL 799.453363 52.88444 \nL 799.559037 54.24763 \nL 799.590739 51.28906 \nL 799.654143 55.992811 \nL 799.691129 57.696798 \nL 799.712264 55.266276 \nL 799.743966 55.386117 \nL 799.817938 51.584917 \nL 799.823221 51.34898 \nL 799.839072 52.944361 \nL 799.860207 52.644759 \nL 799.865491 55.09775 \nL 799.95003 49.967066 \nL 799.965881 50.405234 \nL 800.097973 59.5019 \nL 800.119108 55.756875 \nL 800.124391 54.891774 \nL 800.145526 57.30357 \nL 800.224781 56.606996 \nL 800.261767 52.8395 \nL 800.378008 46.409292 \nL 800.298753 52.929381 \nL 800.383292 46.993516 \nL 800.393859 46.285706 \nL 800.404427 46.514152 \nL 806.031554 38.679559 \nL 806.110809 32.91971 \nL 806.153079 35.320272 \nL 806.184781 36.818282 \nL 806.200632 40.634463 \nL 806.301022 39.338684 \nL 806.422547 43.424506 \nL 806.364426 39.248803 \nL 806.438398 42.619326 \nL 806.501802 39.892948 \nL 806.554639 40.570797 \nL 806.565206 41.237412 \nL 806.618043 39.803067 \nL 806.655029 40.27494 \nL 806.802972 35.938201 \nL 806.81354 35.983141 \nL 806.956199 39.518445 \nL 806.87166 35.166726 \nL 806.987901 38.73199 \nL 807.11471 34.777243 \nL 807.130561 35.582423 \nL 807.162263 34.953259 \nL 807.199249 37.383781 \nL 807.225667 43.18108 \nL 807.326057 39.612071 \nL 807.331341 38.488563 \nL 807.41588 42.065062 \nL 807.426447 47.375508 \nL 807.521554 39.623306 \nL 807.542688 42.136218 \nL 807.606093 37.28641 \nL 807.690632 32.451582 \nL 807.637795 37.694618 \nL 807.738185 34.024493 \nL 807.780454 28.466875 \nL 807.864993 30.781301 \nL 807.912546 34.492621 \nL 807.981234 32.938436 \nL 807.986518 32.635088 \nL 808.007653 35.672304 \nL 808.012937 35.672304 \nL 828.857083 38.934221 \nL 828.878218 26.515717 \nL 828.978608 41.454623 \nL 829.100133 30.114686 \nL 828.989175 41.791675 \nL 829.147686 32.80736 \nL 829.152969 32.717479 \nL 829.158253 33.942102 \nL 829.179388 35.762185 \nL 829.232225 30.912377 \nL 829.269211 34.211744 \nL 829.274494 34.140589 \nL 829.279778 35.383937 \nL 829.285062 34.432701 \nL 829.316764 36.747126 \nL 829.364317 32.87477 \nL 829.396019 35.084335 \nL 829.401303 35.290311 \nL 829.422437 32.781144 \nL 829.427721 32.957161 \nL 829.496409 28.886318 \nL 829.543962 31.534051 \nL 829.628501 36.245293 \nL 829.559813 30.972297 \nL 829.676054 34.3578 \nL 829.723608 36.668481 \nL 829.792296 34.751028 \nL 829.802863 32.313016 \nL 829.845132 36.889437 \nL 829.940239 39.994063 \nL 829.892686 34.732303 \nL 829.961374 38.387447 \nL 829.982508 36.874457 \nL 830.019494 33.762341 \nL 830.093466 35.529993 \nL 830.188572 39.548405 \nL 830.119884 34.971984 \nL 830.214991 38.308802 \nL 830.225558 37.421231 \nL 830.283678 41.75048 \nL 830.320664 38.630874 \nL 830.325948 39.050317 \nL 830.35765 35.589913 \nL 830.410487 37.099159 \nL 830.526728 38.421153 \nL 830.431622 35.406407 \nL 830.537295 37.773263 \nL 830.563714 34.065688 \nL 830.648253 36.054297 \nL 830.833182 42.143708 \nL 830.664104 35.511268 \nL 830.838466 41.870321 \nL 836.465593 50.019496 \nL 836.502578 56.453449 \nL 836.597685 55.666994 \nL 836.602968 55.213846 \nL 836.650522 59.617996 \nL 836.698075 65.864698 \nL 836.766763 61.921186 \nL 836.888288 57.25114 \nL 836.893571 56.831697 \nL 836.951692 60.599193 \nL 836.956976 60.505567 \nL 836.97811 62.864933 \nL 837.015096 59.692896 \nL 837.067933 61.235847 \nL 837.157756 59.475685 \nL 837.110202 62.438 \nL 837.189458 60.228435 \nL 837.369103 73.897777 \nL 837.400805 70.238888 \nL 837.495912 66.793464 \nL 837.517046 67.939442 \nL 837.569883 70.823112 \nL 837.628004 69.15283 \nL 837.670273 72.557058 \nL 837.696692 66.774739 \nL 837.712543 67.205417 \nL 837.786514 68.231554 \nL 837.839351 64.325493 \nL 837.897472 63.707564 \nL 837.960876 65.973304 \nL 837.96616 65.977049 \nL 838.040131 62.992264 \nL 837.982011 65.999519 \nL 838.06655 64.947167 \nL 838.077117 67.973147 \nL 838.140521 64.205652 \nL 838.177507 66.647408 \nL 838.336018 54.386196 \nL 838.357153 54.648347 \nL 838.378287 56.063967 \nL 838.404706 52.689699 \nL 838.446975 54.303805 \nL 844.074102 38.117806 \nL 844.08467 36.174137 \nL 844.116372 41.424663 \nL 844.174492 41.147531 \nL 844.200911 46.143395 \nL 844.28545 41.960202 \nL 844.290733 40.776773 \nL 844.34357 44.66411 \nL 844.38584 43.342116 \nL 844.44396 44.252157 \nL 844.454528 42.031357 \nL 844.475663 42.484505 \nL 844.496797 40.106414 \nL 844.565485 44.870086 \nL 844.618322 48.521486 \nL 844.68701 47.824911 \nL 844.697577 48.000927 \nL 844.782116 50.948262 \nL 844.813819 49.468977 \nL 844.824386 48.199414 \nL 844.893074 53.57727 \nL 844.898358 54.479821 \nL 844.935343 49.689934 \nL 844.967045 52.07177 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 46.0125 349.92 \nL 46.0125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 883.0125 349.92 \nL 883.0125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 46.0125 349.92 \nL 883.0125 349.92 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 46.0125 7.2 \nL 883.0125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p51cb4b8e3e\">\n   <rect height=\"342.72\" width=\"837\" x=\"46.0125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:35.306620Z",
     "start_time": "2019-10-02T17:27:34.835593Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                         Open      High       Low     Close  Volume\n",
       "Datetime                                                           \n",
       "2021-05-10 11:53:00  33203.80  33206.50  33199.75  33201.60       0\n",
       "2021-05-10 11:54:00  33203.35  33214.25  33198.50  33204.85       0\n",
       "2021-05-10 11:55:00  33205.15  33208.60  33201.90  33205.15       0\n",
       "2021-05-10 11:56:00  33206.00  33206.00  33185.00  33189.10       0\n",
       "2021-05-10 11:57:00  33188.40  33193.95  33177.00  33179.00       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-05-10 11:53:00</th>\n",
       "      <td>33203.80</td>\n",
       "      <td>33206.50</td>\n",
       "      <td>33199.75</td>\n",
       "      <td>33201.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-10 11:54:00</th>\n",
       "      <td>33203.35</td>\n",
       "      <td>33214.25</td>\n",
       "      <td>33198.50</td>\n",
       "      <td>33204.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-10 11:55:00</th>\n",
       "      <td>33205.15</td>\n",
       "      <td>33208.60</td>\n",
       "      <td>33201.90</td>\n",
       "      <td>33205.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-10 11:56:00</th>\n",
       "      <td>33206.00</td>\n",
       "      <td>33206.00</td>\n",
       "      <td>33185.00</td>\n",
       "      <td>33189.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-10 11:57:00</th>\n",
       "      <td>33188.40</td>\n",
       "      <td>33193.95</td>\n",
       "      <td>33177.00</td>\n",
       "      <td>33179.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "initial_balance1 = data.between_time(start_time = '09:16:00', end_time = '10:15:00', include_end = True)\n",
    "initial_balance2 = data.between_time(start_time = '14:31:00', end_time = '15:30:00', include_end = True)\n",
    "initial_balance = pd.concat([initial_balance1,initial_balance2],axis=0)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:35.328621Z",
     "start_time": "2019-10-02T17:27:35.308620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "initial_balance.dropna(inplace=True)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:35.877653Z",
     "start_time": "2019-10-02T17:27:35.864652Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "initial_balance.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                         Open      High       Low     Close  Volume\n",
       "Datetime                                                           \n",
       "2021-05-11 09:16:00  32696.00  32698.50  32665.25  32696.45       0\n",
       "2021-05-11 09:17:00  32697.95  32702.80  32670.20  32701.05       0\n",
       "2021-05-11 09:18:00  32698.45  32728.80  32698.45  32718.45       0\n",
       "2021-05-11 09:19:00  32715.20  32721.05  32694.00  32721.05       0\n",
       "2021-05-11 09:20:00  32711.65  32728.90  32686.05  32728.80       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-05-11 09:16:00</th>\n",
       "      <td>32696.00</td>\n",
       "      <td>32698.50</td>\n",
       "      <td>32665.25</td>\n",
       "      <td>32696.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-11 09:17:00</th>\n",
       "      <td>32697.95</td>\n",
       "      <td>32702.80</td>\n",
       "      <td>32670.20</td>\n",
       "      <td>32701.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-11 09:18:00</th>\n",
       "      <td>32698.45</td>\n",
       "      <td>32728.80</td>\n",
       "      <td>32698.45</td>\n",
       "      <td>32718.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-11 09:19:00</th>\n",
       "      <td>32715.20</td>\n",
       "      <td>32721.05</td>\n",
       "      <td>32694.00</td>\n",
       "      <td>32721.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-11 09:20:00</th>\n",
       "      <td>32711.65</td>\n",
       "      <td>32728.90</td>\n",
       "      <td>32686.05</td>\n",
       "      <td>32728.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:36.669698Z",
     "start_time": "2019-10-02T17:27:36.647697Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "eod_returns = data.between_time(start_time = '09:16:00', end_time = '15:30:00', include_end = True)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:37.406740Z",
     "start_time": "2019-10-02T17:27:37.385739Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "conversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last'}\n",
    "data2 = eod_returns.resample('1D').agg(conversion)\n",
    "data2['target'] = data2['Open']/data2['Close'].shift(1)\n",
    "data2.dropna(inplace=True)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:38.278790Z",
     "start_time": "2019-10-02T17:27:38.161783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data2.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                low     open    close     high    target\n",
       "date                                                    \n",
       "2018-01-02  25232.8  25387.0  25319.5  25425.5  1.004566\n",
       "2018-01-03  25300.9  25433.2  25326.2  25454.9  1.004491\n",
       "2018-01-04  25310.3  25356.1  25478.4  25490.3  1.001181\n",
       "2018-01-05  25499.6  25510.2  25619.9  25643.3  1.001248\n",
       "2018-01-09  25617.3  25737.4  25688.0  25803.8  1.002626"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>25232.8</td>\n",
       "      <td>25387.0</td>\n",
       "      <td>25319.5</td>\n",
       "      <td>25425.5</td>\n",
       "      <td>1.004566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>25300.9</td>\n",
       "      <td>25433.2</td>\n",
       "      <td>25326.2</td>\n",
       "      <td>25454.9</td>\n",
       "      <td>1.004491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>25310.3</td>\n",
       "      <td>25356.1</td>\n",
       "      <td>25478.4</td>\n",
       "      <td>25490.3</td>\n",
       "      <td>1.001181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>25499.6</td>\n",
       "      <td>25510.2</td>\n",
       "      <td>25619.9</td>\n",
       "      <td>25643.3</td>\n",
       "      <td>1.001248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>25617.3</td>\n",
       "      <td>25737.4</td>\n",
       "      <td>25688.0</td>\n",
       "      <td>25803.8</td>\n",
       "      <td>1.002626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:39.198843Z",
     "start_time": "2019-10-02T17:27:39.181842Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:27:40.242902Z",
     "start_time": "2019-10-02T17:27:40.237902Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "X = []\n",
    "y = []\n",
    "for single_date in daterange(start, end+timedelta(1)):\n",
    "    if single_date.strftime(\"%Y-%m-%d\") in data2.index:\n",
    "        curr = single_date.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        Xdelta = []\n",
    "        for i in range(0,initial_balance.loc[curr].shape[0]):\n",
    "            \n",
    "            for j in range(0,4):\n",
    "                Xdelta.append(initial_balance.loc[curr].iloc[i][j])\n",
    "        print(curr,' ',len(Xdelta))\n",
    "        if(len(Xdelta) == 480):\n",
    "            X.append(Xdelta)\n",
    "            y.append(data2.loc[curr].loc['target'])\n",
    "        else:\n",
    "            print('Excluded - ',curr,' Length: ',len(Xdelta))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-05-11   480\n",
      "2021-05-12   480\n",
      "2021-05-18   480\n",
      "2021-05-19   480\n",
      "2021-05-20   480\n",
      "2021-05-21   480\n",
      "2021-05-25   480\n",
      "2021-05-26   480\n",
      "2021-05-27   480\n",
      "2021-05-28   480\n",
      "2021-06-01   480\n",
      "2021-06-02   480\n",
      "2021-06-03   480\n",
      "2021-06-04   480\n",
      "2021-06-08   480\n",
      "2021-06-09   480\n",
      "2021-06-10   480\n",
      "2021-06-11   480\n",
      "2021-06-15   480\n",
      "2021-06-16   480\n",
      "2021-06-17   480\n",
      "2021-06-18   480\n",
      "2021-06-22   480\n",
      "2021-06-23   480\n",
      "2021-06-24   480\n",
      "2021-06-25   480\n",
      "2021-06-29   480\n",
      "2021-06-30   480\n",
      "2021-07-01   480\n",
      "2021-07-02   480\n",
      "2021-07-06   480\n",
      "2021-07-07   480\n",
      "2021-07-08   480\n",
      "2021-07-09   480\n",
      "2021-07-13   480\n",
      "2021-07-14   480\n",
      "2021-07-15   480\n",
      "2021-07-16   480\n",
      "2021-07-20   480\n",
      "2021-07-23   480\n",
      "2021-07-27   480\n",
      "2021-07-28   480\n",
      "2021-07-29   480\n",
      "2021-07-30   480\n",
      "2021-08-03   480\n",
      "2021-08-04   480\n",
      "2021-08-05   480\n",
      "2021-08-06   480\n",
      "2021-08-10   480\n",
      "2021-08-11   480\n",
      "2021-08-12   480\n",
      "2021-08-13   480\n",
      "2021-08-17   480\n",
      "2021-08-18   240\n",
      "Excluded -  2021-08-18  Length:  240\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:30:14.063700Z",
     "start_time": "2019-10-02T17:27:41.257960Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "print('Dimension of X:',len(X),'x',len(X[0]))\n",
    "print('Dimension of y:',len(y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dimension of X: 53 x 480\n",
      "Dimension of y: 53\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:08.391808Z",
     "start_time": "2019-10-02T17:31:08.384807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "cols = []\n",
    "for i in range(0,120):\n",
    "    for j in range(0,4):\n",
    "        cols.append('f'+str(i)+str(j))\n",
    "len(cols)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:08.717826Z",
     "start_time": "2019-10-02T17:31:08.707826Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "y = pd.Series(y)\n",
    "X = pd.DataFrame(np.reshape(X,(y.shape[0],len(cols))), columns=cols)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:08.974841Z",
     "start_time": "2019-10-02T17:31:08.936839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "target = pd.DataFrame()\n",
    "target['Close'] = y"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:09.117849Z",
     "start_time": "2019-10-02T17:31:09.109849Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "target.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(53, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:09.293859Z",
     "start_time": "2019-10-02T17:31:09.285859Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "target['dir'] = 0\n",
    "target['dir'] = np.where(target['Close'] > 1, 1, 0)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:09.565875Z",
     "start_time": "2019-10-02T17:31:09.557875Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "y = pd.Series(target['dir'])\n",
    "y.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(53,)"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:10.584933Z",
     "start_time": "2019-10-02T17:31:10.578933Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "X_y = pd.concat([X, y], axis=1)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:11.008958Z",
     "start_time": "2019-10-02T17:31:11.001957Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "X_y.dropna(inplace=True)\n",
    "X = X_y.iloc[:,:-1]\n",
    "y = X_y.iloc[:,-1]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:11.453983Z",
     "start_time": "2019-10-02T17:31:11.440982Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "print(sum(y == 0))\n",
    "print(sum(y == 1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19\n",
      "34\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:12.463041Z",
     "start_time": "2019-10-02T17:31:12.456040Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "X_balanced,y_balanced = balanceData(X,y)\n",
    "print(sum(y_balanced == 0))\n",
    "print(sum(y_balanced == 1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n",
      "34\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:13.657109Z",
     "start_time": "2019-10-02T17:31:13.622107Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "print('Dimension of X:',X.shape)\n",
    "print('Dimension of y:',len(y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dimension of X: (53, 480)\n",
      "Dimension of y: 53\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:19.421439Z",
     "start_time": "2019-10-02T17:31:19.416438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced,y_balanced, test_size=0.2, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:19.629451Z",
     "start_time": "2019-10-02T17:31:19.619450Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(54, 480)\n",
      "(14, 480)\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:20.445497Z",
     "start_time": "2019-10-02T17:31:20.440497Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "print(sum(y_train == 0))\n",
    "print(sum(y_train == 1))\n",
    "print(sum(y_test == 0))\n",
    "print(sum(y_test == 1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26\n",
      "28\n",
      "8\n",
      "6\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:21.342549Z",
     "start_time": "2019-10-02T17:31:21.334548Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:31:22.485614Z",
     "start_time": "2019-10-02T17:31:22.467613Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "unit = 1024\n",
    "optimizer = SGD(lr=0.001, momentum=0.0, nesterov=True)\n",
    "kernel_init = 'he_uniform'\n",
    "activation = 'relu'   \n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('./models/bestmodel.h5', verbose=1, monitor='val_acc',save_best_only=True, mode='auto')\n",
    "stoppoint = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', verbose=0, patience=200)\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 512, kernel_initializer = kernel_init, activation = activation,input_dim = X_train_scaled.shape[1]))\n",
    "classifier.add(Dense(units = unit, kernel_initializer = kernel_init, activation = activation))\n",
    "classifier.add(Dense(units = unit, kernel_initializer = kernel_init, activation = activation))\n",
    "classifier.add(Dense(units = unit, kernel_initializer = kernel_init, activation = activation))\n",
    "classifier.add(Dense(units = unit, kernel_initializer = kernel_init, activation = activation))\n",
    "classifier.add(Dense(units = unit, kernel_initializer = kernel_init, activation = activation))\n",
    "classifier.add(Dense(units = unit//2, kernel_initializer = kernel_init, activation = activation))\n",
    "classifier.add(Dense(units = 1, kernel_initializer = kernel_init, activation = 'sigmoid'))\n",
    "classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(X_train_scaled, y_train, batch_size = 50, epochs = 1000, \n",
    "               validation_split = 0.3 , callbacks=[checkpoint, stoppoint],verbose=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rahul/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.8188 - accuracy: 0.5405 - val_loss: 0.9325 - val_accuracy: 0.3529\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.7478 - accuracy: 0.5405 - val_loss: 0.8331 - val_accuracy: 0.3529\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7087 - accuracy: 0.5405 - val_loss: 0.7765 - val_accuracy: 0.3529\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6892 - accuracy: 0.5405 - val_loss: 0.7430 - val_accuracy: 0.3529\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6786 - accuracy: 0.5676 - val_loss: 0.7209 - val_accuracy: 0.3529\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6715 - accuracy: 0.5676 - val_loss: 0.7060 - val_accuracy: 0.3529\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6660 - accuracy: 0.5946 - val_loss: 0.6954 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6614 - accuracy: 0.5946 - val_loss: 0.6874 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6572 - accuracy: 0.6486 - val_loss: 0.6814 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6534 - accuracy: 0.6216 - val_loss: 0.6763 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6497 - accuracy: 0.6216 - val_loss: 0.6721 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6462 - accuracy: 0.6216 - val_loss: 0.6685 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6429 - accuracy: 0.6216 - val_loss: 0.6650 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6397 - accuracy: 0.6486 - val_loss: 0.6623 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6366 - accuracy: 0.6486 - val_loss: 0.6597 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.6337 - accuracy: 0.6486 - val_loss: 0.6570 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6308 - accuracy: 0.6486 - val_loss: 0.6553 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.6280 - accuracy: 0.6486 - val_loss: 0.6532 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.6253 - accuracy: 0.7027 - val_loss: 0.6513 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6226 - accuracy: 0.7027 - val_loss: 0.6494 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6200 - accuracy: 0.7027 - val_loss: 0.6476 - val_accuracy: 0.4118\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6175 - accuracy: 0.7297 - val_loss: 0.6459 - val_accuracy: 0.4706\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6149 - accuracy: 0.7568 - val_loss: 0.6441 - val_accuracy: 0.4706\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6124 - accuracy: 0.7568 - val_loss: 0.6425 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6099 - accuracy: 0.7568 - val_loss: 0.6411 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6074 - accuracy: 0.7838 - val_loss: 0.6396 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6050 - accuracy: 0.7838 - val_loss: 0.6383 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6026 - accuracy: 0.7838 - val_loss: 0.6376 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6004 - accuracy: 0.7838 - val_loss: 0.6359 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5982 - accuracy: 0.7568 - val_loss: 0.6351 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5961 - accuracy: 0.7838 - val_loss: 0.6337 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5940 - accuracy: 0.7838 - val_loss: 0.6327 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5919 - accuracy: 0.7838 - val_loss: 0.6321 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5899 - accuracy: 0.7838 - val_loss: 0.6306 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5879 - accuracy: 0.8108 - val_loss: 0.6295 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5860 - accuracy: 0.8108 - val_loss: 0.6283 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5841 - accuracy: 0.8378 - val_loss: 0.6275 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5822 - accuracy: 0.8649 - val_loss: 0.6263 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5804 - accuracy: 0.8649 - val_loss: 0.6253 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5786 - accuracy: 0.8649 - val_loss: 0.6245 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5767 - accuracy: 0.8649 - val_loss: 0.6235 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5749 - accuracy: 0.8649 - val_loss: 0.6226 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5730 - accuracy: 0.8649 - val_loss: 0.6221 - val_accuracy: 0.5294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.5712 - accuracy: 0.8649 - val_loss: 0.6209 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5695 - accuracy: 0.8649 - val_loss: 0.6203 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5678 - accuracy: 0.8649 - val_loss: 0.6190 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5661 - accuracy: 0.8649 - val_loss: 0.6185 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5644 - accuracy: 0.8649 - val_loss: 0.6179 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5628 - accuracy: 0.8649 - val_loss: 0.6167 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5611 - accuracy: 0.8649 - val_loss: 0.6163 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5594 - accuracy: 0.8649 - val_loss: 0.6154 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5578 - accuracy: 0.8649 - val_loss: 0.6150 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.5562 - accuracy: 0.8649 - val_loss: 0.6142 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5546 - accuracy: 0.8649 - val_loss: 0.6136 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5530 - accuracy: 0.8649 - val_loss: 0.6130 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5515 - accuracy: 0.8649 - val_loss: 0.6116 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5500 - accuracy: 0.8649 - val_loss: 0.6110 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5486 - accuracy: 0.8649 - val_loss: 0.6101 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5472 - accuracy: 0.8649 - val_loss: 0.6090 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5458 - accuracy: 0.8649 - val_loss: 0.6084 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5444 - accuracy: 0.8649 - val_loss: 0.6075 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5430 - accuracy: 0.8649 - val_loss: 0.6066 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5417 - accuracy: 0.8649 - val_loss: 0.6057 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5403 - accuracy: 0.8649 - val_loss: 0.6046 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5390 - accuracy: 0.8649 - val_loss: 0.6041 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5376 - accuracy: 0.8649 - val_loss: 0.6030 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5363 - accuracy: 0.8919 - val_loss: 0.6026 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5350 - accuracy: 0.8919 - val_loss: 0.6014 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.5337 - accuracy: 0.8919 - val_loss: 0.6011 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5324 - accuracy: 0.8919 - val_loss: 0.5999 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5311 - accuracy: 0.8919 - val_loss: 0.5994 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5298 - accuracy: 0.8919 - val_loss: 0.5988 - val_accuracy: 0.5882\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5285 - accuracy: 0.8919 - val_loss: 0.5985 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5272 - accuracy: 0.8919 - val_loss: 0.5976 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5260 - accuracy: 0.8919 - val_loss: 0.5973 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5247 - accuracy: 0.9189 - val_loss: 0.5965 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5234 - accuracy: 0.9189 - val_loss: 0.5962 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5222 - accuracy: 0.9189 - val_loss: 0.5955 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5210 - accuracy: 0.9189 - val_loss: 0.5950 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5198 - accuracy: 0.9189 - val_loss: 0.5944 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5186 - accuracy: 0.9189 - val_loss: 0.5940 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5174 - accuracy: 0.9189 - val_loss: 0.5935 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5163 - accuracy: 0.9189 - val_loss: 0.5926 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5152 - accuracy: 0.9189 - val_loss: 0.5923 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5141 - accuracy: 0.9189 - val_loss: 0.5918 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5130 - accuracy: 0.9189 - val_loss: 0.5916 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5119 - accuracy: 0.9189 - val_loss: 0.5909 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5108 - accuracy: 0.9189 - val_loss: 0.5905 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5098 - accuracy: 0.9189 - val_loss: 0.5902 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5088 - accuracy: 0.9189 - val_loss: 0.5897 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5077 - accuracy: 0.9189 - val_loss: 0.5892 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5067 - accuracy: 0.9189 - val_loss: 0.5889 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5057 - accuracy: 0.9189 - val_loss: 0.5885 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5047 - accuracy: 0.9189 - val_loss: 0.5879 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5037 - accuracy: 0.9189 - val_loss: 0.5875 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5027 - accuracy: 0.9189 - val_loss: 0.5867 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5017 - accuracy: 0.9189 - val_loss: 0.5863 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5007 - accuracy: 0.9189 - val_loss: 0.5859 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4997 - accuracy: 0.9189 - val_loss: 0.5856 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4988 - accuracy: 0.9189 - val_loss: 0.5851 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.4978 - accuracy: 0.9189 - val_loss: 0.5845 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4968 - accuracy: 0.9189 - val_loss: 0.5846 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4958 - accuracy: 0.9189 - val_loss: 0.5840 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4949 - accuracy: 0.9189 - val_loss: 0.5838 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4939 - accuracy: 0.9189 - val_loss: 0.5826 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4930 - accuracy: 0.9189 - val_loss: 0.5828 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4920 - accuracy: 0.9189 - val_loss: 0.5820 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4911 - accuracy: 0.9189 - val_loss: 0.5816 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4902 - accuracy: 0.9189 - val_loss: 0.5814 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4893 - accuracy: 0.9189 - val_loss: 0.5805 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.4884 - accuracy: 0.9189 - val_loss: 0.5804 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4875 - accuracy: 0.9189 - val_loss: 0.5800 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4866 - accuracy: 0.9189 - val_loss: 0.5795 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4857 - accuracy: 0.9189 - val_loss: 0.5792 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4849 - accuracy: 0.9189 - val_loss: 0.5790 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4840 - accuracy: 0.9189 - val_loss: 0.5786 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4831 - accuracy: 0.9189 - val_loss: 0.5784 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4823 - accuracy: 0.9189 - val_loss: 0.5781 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4814 - accuracy: 0.9189 - val_loss: 0.5784 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4806 - accuracy: 0.9189 - val_loss: 0.5777 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4798 - accuracy: 0.9189 - val_loss: 0.5775 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4789 - accuracy: 0.9189 - val_loss: 0.5774 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4781 - accuracy: 0.9189 - val_loss: 0.5772 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4772 - accuracy: 0.9189 - val_loss: 0.5773 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4765 - accuracy: 0.9189 - val_loss: 0.5770 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4756 - accuracy: 0.9189 - val_loss: 0.5771 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4748 - accuracy: 0.9189 - val_loss: 0.5767 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4740 - accuracy: 0.9189 - val_loss: 0.5762 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4733 - accuracy: 0.9189 - val_loss: 0.5763 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4725 - accuracy: 0.9189 - val_loss: 0.5759 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4717 - accuracy: 0.9189 - val_loss: 0.5761 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4709 - accuracy: 0.9189 - val_loss: 0.5755 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4702 - accuracy: 0.9189 - val_loss: 0.5756 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4695 - accuracy: 0.9189 - val_loss: 0.5752 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4687 - accuracy: 0.9189 - val_loss: 0.5751 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4679 - accuracy: 0.9189 - val_loss: 0.5748 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4672 - accuracy: 0.9189 - val_loss: 0.5747 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4664 - accuracy: 0.9189 - val_loss: 0.5742 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4657 - accuracy: 0.9189 - val_loss: 0.5741 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4649 - accuracy: 0.9189 - val_loss: 0.5742 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4642 - accuracy: 0.9189 - val_loss: 0.5734 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4635 - accuracy: 0.9189 - val_loss: 0.5737 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4627 - accuracy: 0.9189 - val_loss: 0.5730 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4620 - accuracy: 0.9189 - val_loss: 0.5729 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4612 - accuracy: 0.9189 - val_loss: 0.5728 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4605 - accuracy: 0.9189 - val_loss: 0.5723 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4598 - accuracy: 0.9189 - val_loss: 0.5720 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.4591 - accuracy: 0.9189 - val_loss: 0.5721 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4584 - accuracy: 0.9189 - val_loss: 0.5719 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4577 - accuracy: 0.9189 - val_loss: 0.5715 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4570 - accuracy: 0.9189 - val_loss: 0.5711 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4563 - accuracy: 0.9189 - val_loss: 0.5712 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4555 - accuracy: 0.9189 - val_loss: 0.5701 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4549 - accuracy: 0.9189 - val_loss: 0.5707 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4541 - accuracy: 0.9189 - val_loss: 0.5702 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4534 - accuracy: 0.9189 - val_loss: 0.5702 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4528 - accuracy: 0.9189 - val_loss: 0.5696 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4520 - accuracy: 0.9189 - val_loss: 0.5699 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4514 - accuracy: 0.9189 - val_loss: 0.5694 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4507 - accuracy: 0.9189 - val_loss: 0.5702 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4499 - accuracy: 0.9189 - val_loss: 0.5688 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4493 - accuracy: 0.9189 - val_loss: 0.5695 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4486 - accuracy: 0.9189 - val_loss: 0.5688 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4479 - accuracy: 0.9189 - val_loss: 0.5688 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4473 - accuracy: 0.9189 - val_loss: 0.5681 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4466 - accuracy: 0.9189 - val_loss: 0.5684 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4460 - accuracy: 0.9189 - val_loss: 0.5676 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4453 - accuracy: 0.9189 - val_loss: 0.5681 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4446 - accuracy: 0.9189 - val_loss: 0.5674 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4440 - accuracy: 0.9189 - val_loss: 0.5676 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4433 - accuracy: 0.9189 - val_loss: 0.5671 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4426 - accuracy: 0.9189 - val_loss: 0.5669 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4420 - accuracy: 0.9189 - val_loss: 0.5666 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4413 - accuracy: 0.9189 - val_loss: 0.5669 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4407 - accuracy: 0.9189 - val_loss: 0.5661 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4400 - accuracy: 0.9189 - val_loss: 0.5659 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4394 - accuracy: 0.9189 - val_loss: 0.5656 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4387 - accuracy: 0.9189 - val_loss: 0.5651 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4381 - accuracy: 0.9189 - val_loss: 0.5651 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4375 - accuracy: 0.9189 - val_loss: 0.5652 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4368 - accuracy: 0.9189 - val_loss: 0.5647 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4362 - accuracy: 0.9189 - val_loss: 0.5648 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4356 - accuracy: 0.9189 - val_loss: 0.5641 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4349 - accuracy: 0.9189 - val_loss: 0.5640 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4343 - accuracy: 0.9189 - val_loss: 0.5644 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4337 - accuracy: 0.9189 - val_loss: 0.5633 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4330 - accuracy: 0.9189 - val_loss: 0.5638 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4324 - accuracy: 0.9189 - val_loss: 0.5628 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4318 - accuracy: 0.9189 - val_loss: 0.5634 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4312 - accuracy: 0.9189 - val_loss: 0.5632 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4306 - accuracy: 0.9189 - val_loss: 0.5630 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4300 - accuracy: 0.9189 - val_loss: 0.5625 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4294 - accuracy: 0.9189 - val_loss: 0.5633 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4288 - accuracy: 0.9189 - val_loss: 0.5624 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4282 - accuracy: 0.9189 - val_loss: 0.5622 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4276 - accuracy: 0.9189 - val_loss: 0.5618 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4270 - accuracy: 0.9189 - val_loss: 0.5625 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4264 - accuracy: 0.9189 - val_loss: 0.5623 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4258 - accuracy: 0.9189 - val_loss: 0.5616 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4252 - accuracy: 0.9189 - val_loss: 0.5619 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4246 - accuracy: 0.9189 - val_loss: 0.5620 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4240 - accuracy: 0.9189 - val_loss: 0.5615 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4235 - accuracy: 0.9189 - val_loss: 0.5616 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4228 - accuracy: 0.9189 - val_loss: 0.5612 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4223 - accuracy: 0.9189 - val_loss: 0.5611 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4218 - accuracy: 0.9189 - val_loss: 0.5609 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4212 - accuracy: 0.9189 - val_loss: 0.5607 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4206 - accuracy: 0.9189 - val_loss: 0.5607 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4200 - accuracy: 0.9189 - val_loss: 0.5601 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4195 - accuracy: 0.9189 - val_loss: 0.5603 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4189 - accuracy: 0.9189 - val_loss: 0.5609 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4183 - accuracy: 0.9189 - val_loss: 0.5600 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4178 - accuracy: 0.9189 - val_loss: 0.5601 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4171 - accuracy: 0.9189 - val_loss: 0.5598 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4166 - accuracy: 0.9189 - val_loss: 0.5600 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4160 - accuracy: 0.9189 - val_loss: 0.5594 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4155 - accuracy: 0.9189 - val_loss: 0.5594 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4149 - accuracy: 0.9189 - val_loss: 0.5592 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4143 - accuracy: 0.9189 - val_loss: 0.5589 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.4138 - accuracy: 0.9189 - val_loss: 0.5592 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4132 - accuracy: 0.9189 - val_loss: 0.5588 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4126 - accuracy: 0.9189 - val_loss: 0.5584 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4120 - accuracy: 0.9189 - val_loss: 0.5583 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4114 - accuracy: 0.9189 - val_loss: 0.5584 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4109 - accuracy: 0.9189 - val_loss: 0.5583 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4103 - accuracy: 0.9189 - val_loss: 0.5582 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4097 - accuracy: 0.9189 - val_loss: 0.5583 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4092 - accuracy: 0.9189 - val_loss: 0.5579 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4086 - accuracy: 0.9189 - val_loss: 0.5576 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4080 - accuracy: 0.9189 - val_loss: 0.5581 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4075 - accuracy: 0.9189 - val_loss: 0.5571 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4069 - accuracy: 0.9189 - val_loss: 0.5580 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4064 - accuracy: 0.9189 - val_loss: 0.5565 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4058 - accuracy: 0.9189 - val_loss: 0.5576 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4052 - accuracy: 0.9189 - val_loss: 0.5568 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4046 - accuracy: 0.9189 - val_loss: 0.5574 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4042 - accuracy: 0.9189 - val_loss: 0.5566 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4035 - accuracy: 0.9189 - val_loss: 0.5575 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4030 - accuracy: 0.9189 - val_loss: 0.5565 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4024 - accuracy: 0.9189 - val_loss: 0.5570 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4019 - accuracy: 0.9189 - val_loss: 0.5565 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4013 - accuracy: 0.9189 - val_loss: 0.5562 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4007 - accuracy: 0.9189 - val_loss: 0.5564 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4002 - accuracy: 0.9189 - val_loss: 0.5563 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3996 - accuracy: 0.9189 - val_loss: 0.5562 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3990 - accuracy: 0.9189 - val_loss: 0.5563 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3985 - accuracy: 0.9189 - val_loss: 0.5560 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3980 - accuracy: 0.9189 - val_loss: 0.5561 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3974 - accuracy: 0.9189 - val_loss: 0.5559 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3968 - accuracy: 0.9189 - val_loss: 0.5562 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3964 - accuracy: 0.9189 - val_loss: 0.5555 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3958 - accuracy: 0.9189 - val_loss: 0.5565 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3953 - accuracy: 0.9189 - val_loss: 0.5554 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3947 - accuracy: 0.9189 - val_loss: 0.5561 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3942 - accuracy: 0.9189 - val_loss: 0.5555 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3937 - accuracy: 0.9189 - val_loss: 0.5559 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3932 - accuracy: 0.9189 - val_loss: 0.5549 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3927 - accuracy: 0.9189 - val_loss: 0.5557 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3922 - accuracy: 0.9189 - val_loss: 0.5552 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3916 - accuracy: 0.9189 - val_loss: 0.5556 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3911 - accuracy: 0.9189 - val_loss: 0.5549 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3906 - accuracy: 0.9189 - val_loss: 0.5554 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3901 - accuracy: 0.9189 - val_loss: 0.5554 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3896 - accuracy: 0.9189 - val_loss: 0.5549 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3891 - accuracy: 0.9189 - val_loss: 0.5554 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3886 - accuracy: 0.9189 - val_loss: 0.5548 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3880 - accuracy: 0.9189 - val_loss: 0.5553 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3875 - accuracy: 0.9189 - val_loss: 0.5550 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3869 - accuracy: 0.9189 - val_loss: 0.5551 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3864 - accuracy: 0.9189 - val_loss: 0.5558 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3858 - accuracy: 0.9189 - val_loss: 0.5551 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3852 - accuracy: 0.9189 - val_loss: 0.5549 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3847 - accuracy: 0.9189 - val_loss: 0.5558 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3841 - accuracy: 0.9189 - val_loss: 0.5550 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3836 - accuracy: 0.9189 - val_loss: 0.5553 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3831 - accuracy: 0.9189 - val_loss: 0.5547 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3825 - accuracy: 0.9189 - val_loss: 0.5543 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3820 - accuracy: 0.9189 - val_loss: 0.5546 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.3814 - accuracy: 0.9189 - val_loss: 0.5545 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3808 - accuracy: 0.9189 - val_loss: 0.5543 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3804 - accuracy: 0.9189 - val_loss: 0.5540 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3799 - accuracy: 0.9189 - val_loss: 0.5537 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3793 - accuracy: 0.9189 - val_loss: 0.5542 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3788 - accuracy: 0.9189 - val_loss: 0.5534 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3782 - accuracy: 0.9189 - val_loss: 0.5539 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3777 - accuracy: 0.9189 - val_loss: 0.5538 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3770 - accuracy: 0.9189 - val_loss: 0.5533 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3765 - accuracy: 0.9189 - val_loss: 0.5544 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3760 - accuracy: 0.9189 - val_loss: 0.5530 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3754 - accuracy: 0.9189 - val_loss: 0.5542 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3749 - accuracy: 0.9189 - val_loss: 0.5534 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3744 - accuracy: 0.9189 - val_loss: 0.5537 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3739 - accuracy: 0.9189 - val_loss: 0.5534 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3734 - accuracy: 0.9189 - val_loss: 0.5532 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3728 - accuracy: 0.9189 - val_loss: 0.5537 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3723 - accuracy: 0.9189 - val_loss: 0.5535 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3718 - accuracy: 0.9189 - val_loss: 0.5534 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3714 - accuracy: 0.9189 - val_loss: 0.5531 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3708 - accuracy: 0.9189 - val_loss: 0.5537 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3703 - accuracy: 0.9189 - val_loss: 0.5533 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3698 - accuracy: 0.9189 - val_loss: 0.5530 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3694 - accuracy: 0.9189 - val_loss: 0.5533 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3689 - accuracy: 0.9189 - val_loss: 0.5530 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3684 - accuracy: 0.9189 - val_loss: 0.5528 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3679 - accuracy: 0.9189 - val_loss: 0.5530 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3674 - accuracy: 0.9189 - val_loss: 0.5528 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.3669 - accuracy: 0.9189 - val_loss: 0.5528 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3665 - accuracy: 0.9189 - val_loss: 0.5529 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3659 - accuracy: 0.9189 - val_loss: 0.5524 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3655 - accuracy: 0.9189 - val_loss: 0.5523 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3651 - accuracy: 0.9189 - val_loss: 0.5525 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3645 - accuracy: 0.9189 - val_loss: 0.5526 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3641 - accuracy: 0.9189 - val_loss: 0.5524 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3636 - accuracy: 0.9189 - val_loss: 0.5525 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3631 - accuracy: 0.9189 - val_loss: 0.5517 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3627 - accuracy: 0.9189 - val_loss: 0.5528 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3622 - accuracy: 0.9189 - val_loss: 0.5518 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3617 - accuracy: 0.9189 - val_loss: 0.5525 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3612 - accuracy: 0.9189 - val_loss: 0.5517 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3607 - accuracy: 0.9189 - val_loss: 0.5527 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3602 - accuracy: 0.9189 - val_loss: 0.5511 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3598 - accuracy: 0.9189 - val_loss: 0.5527 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3592 - accuracy: 0.9189 - val_loss: 0.5516 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3588 - accuracy: 0.9189 - val_loss: 0.5521 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3583 - accuracy: 0.9189 - val_loss: 0.5517 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3578 - accuracy: 0.9189 - val_loss: 0.5520 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3573 - accuracy: 0.9189 - val_loss: 0.5513 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3569 - accuracy: 0.9189 - val_loss: 0.5521 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3564 - accuracy: 0.9189 - val_loss: 0.5514 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3559 - accuracy: 0.9189 - val_loss: 0.5514 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3554 - accuracy: 0.9189 - val_loss: 0.5516 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3549 - accuracy: 0.9459 - val_loss: 0.5514 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3544 - accuracy: 0.9189 - val_loss: 0.5511 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3539 - accuracy: 0.9459 - val_loss: 0.5512 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3534 - accuracy: 0.9189 - val_loss: 0.5511 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.3530 - accuracy: 0.9459 - val_loss: 0.5516 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3525 - accuracy: 0.9459 - val_loss: 0.5514 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3520 - accuracy: 0.9459 - val_loss: 0.5513 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3516 - accuracy: 0.9459 - val_loss: 0.5507 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3511 - accuracy: 0.9459 - val_loss: 0.5515 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3506 - accuracy: 0.9459 - val_loss: 0.5512 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3501 - accuracy: 0.9459 - val_loss: 0.5508 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3496 - accuracy: 0.9459 - val_loss: 0.5515 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3492 - accuracy: 0.9459 - val_loss: 0.5507 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3487 - accuracy: 0.9459 - val_loss: 0.5505 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3483 - accuracy: 0.9459 - val_loss: 0.5514 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3478 - accuracy: 0.9459 - val_loss: 0.5509 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3473 - accuracy: 0.9459 - val_loss: 0.5510 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3469 - accuracy: 0.9459 - val_loss: 0.5512 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3464 - accuracy: 0.9459 - val_loss: 0.5502 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3460 - accuracy: 0.9459 - val_loss: 0.5504 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3455 - accuracy: 0.9459 - val_loss: 0.5506 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3450 - accuracy: 0.9459 - val_loss: 0.5508 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3446 - accuracy: 0.9459 - val_loss: 0.5501 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3441 - accuracy: 0.9459 - val_loss: 0.5506 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3437 - accuracy: 0.9459 - val_loss: 0.5503 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3432 - accuracy: 0.9459 - val_loss: 0.5505 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3428 - accuracy: 0.9459 - val_loss: 0.5498 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3423 - accuracy: 0.9459 - val_loss: 0.5505 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3419 - accuracy: 0.9459 - val_loss: 0.5498 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3415 - accuracy: 0.9459 - val_loss: 0.5497 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3410 - accuracy: 0.9459 - val_loss: 0.5502 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3406 - accuracy: 0.9459 - val_loss: 0.5499 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3401 - accuracy: 0.9459 - val_loss: 0.5498 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3397 - accuracy: 0.9459 - val_loss: 0.5501 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3392 - accuracy: 0.9459 - val_loss: 0.5494 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3388 - accuracy: 0.9459 - val_loss: 0.5502 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3383 - accuracy: 0.9459 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3379 - accuracy: 0.9459 - val_loss: 0.5497 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3375 - accuracy: 0.9459 - val_loss: 0.5494 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3370 - accuracy: 0.9459 - val_loss: 0.5502 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3366 - accuracy: 0.9459 - val_loss: 0.5490 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3362 - accuracy: 0.9459 - val_loss: 0.5501 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3358 - accuracy: 0.9459 - val_loss: 0.5488 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3353 - accuracy: 0.9459 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3349 - accuracy: 0.9459 - val_loss: 0.5492 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3344 - accuracy: 0.9459 - val_loss: 0.5494 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3340 - accuracy: 0.9459 - val_loss: 0.5488 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3336 - accuracy: 0.9459 - val_loss: 0.5495 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3331 - accuracy: 0.9459 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.3327 - accuracy: 0.9459 - val_loss: 0.5493 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3323 - accuracy: 0.9459 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3318 - accuracy: 0.9459 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3314 - accuracy: 0.9459 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3310 - accuracy: 0.9459 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3306 - accuracy: 0.9459 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3302 - accuracy: 0.9459 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3297 - accuracy: 0.9459 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3293 - accuracy: 0.9459 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3290 - accuracy: 0.9459 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3285 - accuracy: 0.9459 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3281 - accuracy: 0.9459 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3277 - accuracy: 0.9459 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3273 - accuracy: 0.9459 - val_loss: 0.5479 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3268 - accuracy: 0.9459 - val_loss: 0.5482 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3264 - accuracy: 0.9459 - val_loss: 0.5488 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3260 - accuracy: 0.9459 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3256 - accuracy: 0.9459 - val_loss: 0.5492 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3252 - accuracy: 0.9459 - val_loss: 0.5478 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3248 - accuracy: 0.9459 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3244 - accuracy: 0.9459 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3240 - accuracy: 0.9459 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3235 - accuracy: 0.9459 - val_loss: 0.5480 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3232 - accuracy: 0.9459 - val_loss: 0.5484 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3228 - accuracy: 0.9459 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3223 - accuracy: 0.9459 - val_loss: 0.5484 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3219 - accuracy: 0.9459 - val_loss: 0.5477 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3215 - accuracy: 0.9459 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3211 - accuracy: 0.9459 - val_loss: 0.5475 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3207 - accuracy: 0.9459 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3203 - accuracy: 0.9459 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3199 - accuracy: 0.9459 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3195 - accuracy: 0.9459 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3191 - accuracy: 0.9459 - val_loss: 0.5479 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3187 - accuracy: 0.9459 - val_loss: 0.5484 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3183 - accuracy: 0.9459 - val_loss: 0.5477 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3179 - accuracy: 0.9459 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3175 - accuracy: 0.9459 - val_loss: 0.5479 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3171 - accuracy: 0.9459 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3167 - accuracy: 0.9459 - val_loss: 0.5475 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3163 - accuracy: 0.9459 - val_loss: 0.5484 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3159 - accuracy: 0.9459 - val_loss: 0.5473 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3155 - accuracy: 0.9459 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3151 - accuracy: 0.9459 - val_loss: 0.5477 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3147 - accuracy: 0.9459 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3143 - accuracy: 0.9459 - val_loss: 0.5478 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3139 - accuracy: 0.9459 - val_loss: 0.5480 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3135 - accuracy: 0.9459 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3131 - accuracy: 0.9459 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3127 - accuracy: 0.9459 - val_loss: 0.5473 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3123 - accuracy: 0.9459 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3119 - accuracy: 0.9459 - val_loss: 0.5475 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3115 - accuracy: 0.9459 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3111 - accuracy: 0.9459 - val_loss: 0.5468 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3107 - accuracy: 0.9459 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3103 - accuracy: 0.9459 - val_loss: 0.5471 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3099 - accuracy: 0.9459 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3095 - accuracy: 0.9459 - val_loss: 0.5467 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3091 - accuracy: 0.9459 - val_loss: 0.5482 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3087 - accuracy: 0.9459 - val_loss: 0.5465 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3084 - accuracy: 0.9459 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3080 - accuracy: 0.9459 - val_loss: 0.5467 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3076 - accuracy: 0.9459 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3072 - accuracy: 0.9459 - val_loss: 0.5469 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3068 - accuracy: 0.9459 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3064 - accuracy: 0.9459 - val_loss: 0.5470 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3060 - accuracy: 0.9459 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3056 - accuracy: 0.9459 - val_loss: 0.5470 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3052 - accuracy: 0.9459 - val_loss: 0.5479 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3048 - accuracy: 0.9459 - val_loss: 0.5467 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3045 - accuracy: 0.9459 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3041 - accuracy: 0.9459 - val_loss: 0.5468 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3036 - accuracy: 0.9459 - val_loss: 0.5482 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3033 - accuracy: 0.9459 - val_loss: 0.5461 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3029 - accuracy: 0.9459 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3025 - accuracy: 0.9459 - val_loss: 0.5466 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3021 - accuracy: 0.9459 - val_loss: 0.5482 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3017 - accuracy: 0.9459 - val_loss: 0.5464 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3013 - accuracy: 0.9459 - val_loss: 0.5479 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3010 - accuracy: 0.9459 - val_loss: 0.5460 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3006 - accuracy: 0.9459 - val_loss: 0.5484 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3002 - accuracy: 0.9459 - val_loss: 0.5469 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2998 - accuracy: 0.9459 - val_loss: 0.5474 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2994 - accuracy: 0.9459 - val_loss: 0.5464 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2991 - accuracy: 0.9459 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2987 - accuracy: 0.9459 - val_loss: 0.5462 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2983 - accuracy: 0.9459 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2979 - accuracy: 0.9459 - val_loss: 0.5465 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2975 - accuracy: 0.9459 - val_loss: 0.5477 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2972 - accuracy: 0.9459 - val_loss: 0.5462 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2968 - accuracy: 0.9459 - val_loss: 0.5470 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2964 - accuracy: 0.9459 - val_loss: 0.5469 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2960 - accuracy: 0.9459 - val_loss: 0.5475 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2957 - accuracy: 0.9459 - val_loss: 0.5472 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2953 - accuracy: 0.9459 - val_loss: 0.5473 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2949 - accuracy: 0.9459 - val_loss: 0.5468 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2946 - accuracy: 0.9459 - val_loss: 0.5471 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2941 - accuracy: 0.9459 - val_loss: 0.5471 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2938 - accuracy: 0.9459 - val_loss: 0.5469 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2934 - accuracy: 0.9730 - val_loss: 0.5475 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2930 - accuracy: 0.9459 - val_loss: 0.5471 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2926 - accuracy: 0.9730 - val_loss: 0.5471 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2923 - accuracy: 0.9459 - val_loss: 0.5472 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2919 - accuracy: 0.9730 - val_loss: 0.5474 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2915 - accuracy: 0.9459 - val_loss: 0.5475 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2911 - accuracy: 0.9730 - val_loss: 0.5471 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2908 - accuracy: 0.9730 - val_loss: 0.5474 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2904 - accuracy: 0.9730 - val_loss: 0.5469 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2900 - accuracy: 0.9730 - val_loss: 0.5478 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2897 - accuracy: 0.9730 - val_loss: 0.5472 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2893 - accuracy: 0.9730 - val_loss: 0.5472 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2889 - accuracy: 0.9730 - val_loss: 0.5474 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2885 - accuracy: 0.9730 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2882 - accuracy: 0.9730 - val_loss: 0.5469 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2878 - accuracy: 0.9730 - val_loss: 0.5478 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2874 - accuracy: 0.9730 - val_loss: 0.5468 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2871 - accuracy: 0.9730 - val_loss: 0.5473 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2867 - accuracy: 0.9730 - val_loss: 0.5478 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2863 - accuracy: 0.9730 - val_loss: 0.5468 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2860 - accuracy: 0.9730 - val_loss: 0.5478 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2856 - accuracy: 0.9730 - val_loss: 0.5469 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2853 - accuracy: 0.9730 - val_loss: 0.5471 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2849 - accuracy: 0.9730 - val_loss: 0.5472 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2845 - accuracy: 0.9730 - val_loss: 0.5474 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2841 - accuracy: 0.9730 - val_loss: 0.5474 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2838 - accuracy: 0.9730 - val_loss: 0.5474 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2834 - accuracy: 0.9730 - val_loss: 0.5474 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2830 - accuracy: 0.9730 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2827 - accuracy: 0.9730 - val_loss: 0.5471 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2823 - accuracy: 0.9730 - val_loss: 0.5479 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2820 - accuracy: 0.9730 - val_loss: 0.5471 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2816 - accuracy: 0.9730 - val_loss: 0.5482 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2812 - accuracy: 0.9730 - val_loss: 0.5472 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2809 - accuracy: 0.9730 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2805 - accuracy: 0.9730 - val_loss: 0.5467 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2802 - accuracy: 0.9730 - val_loss: 0.5484 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2798 - accuracy: 0.9730 - val_loss: 0.5472 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2795 - accuracy: 0.9730 - val_loss: 0.5479 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2791 - accuracy: 0.9730 - val_loss: 0.5466 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2788 - accuracy: 0.9730 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2784 - accuracy: 0.9730 - val_loss: 0.5459 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2781 - accuracy: 0.9730 - val_loss: 0.5489 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2777 - accuracy: 0.9730 - val_loss: 0.5469 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2773 - accuracy: 0.9730 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2770 - accuracy: 0.9730 - val_loss: 0.5465 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2766 - accuracy: 0.9730 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2763 - accuracy: 0.9730 - val_loss: 0.5466 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2759 - accuracy: 0.9730 - val_loss: 0.5484 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2756 - accuracy: 0.9730 - val_loss: 0.5463 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2753 - accuracy: 0.9730 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2748 - accuracy: 0.9730 - val_loss: 0.5468 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2745 - accuracy: 0.9730 - val_loss: 0.5482 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2742 - accuracy: 0.9730 - val_loss: 0.5479 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2738 - accuracy: 0.9730 - val_loss: 0.5468 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2735 - accuracy: 0.9730 - val_loss: 0.5489 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2731 - accuracy: 0.9730 - val_loss: 0.5467 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2727 - accuracy: 0.9730 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2724 - accuracy: 0.9730 - val_loss: 0.5474 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2721 - accuracy: 0.9730 - val_loss: 0.5495 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2717 - accuracy: 0.9730 - val_loss: 0.5467 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2714 - accuracy: 0.9730 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2710 - accuracy: 0.9730 - val_loss: 0.5472 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2707 - accuracy: 0.9730 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2703 - accuracy: 0.9730 - val_loss: 0.5477 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2700 - accuracy: 0.9730 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2696 - accuracy: 0.9730 - val_loss: 0.5475 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2693 - accuracy: 0.9730 - val_loss: 0.5482 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2690 - accuracy: 0.9730 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2686 - accuracy: 0.9730 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2683 - accuracy: 0.9730 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2679 - accuracy: 0.9730 - val_loss: 0.5477 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2676 - accuracy: 0.9730 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2672 - accuracy: 0.9730 - val_loss: 0.5480 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2669 - accuracy: 0.9730 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2666 - accuracy: 0.9730 - val_loss: 0.5477 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2662 - accuracy: 0.9730 - val_loss: 0.5489 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2659 - accuracy: 0.9730 - val_loss: 0.5480 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2656 - accuracy: 0.9730 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2652 - accuracy: 0.9730 - val_loss: 0.5488 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2649 - accuracy: 0.9730 - val_loss: 0.5480 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2645 - accuracy: 0.9730 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2642 - accuracy: 0.9730 - val_loss: 0.5473 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2638 - accuracy: 0.9730 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2635 - accuracy: 0.9730 - val_loss: 0.5468 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2632 - accuracy: 0.9730 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2629 - accuracy: 0.9730 - val_loss: 0.5476 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2625 - accuracy: 0.9730 - val_loss: 0.5492 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2622 - accuracy: 0.9730 - val_loss: 0.5479 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.2619 - accuracy: 0.9730 - val_loss: 0.5496 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2616 - accuracy: 0.9730 - val_loss: 0.5475 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2612 - accuracy: 0.9730 - val_loss: 0.5495 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2608 - accuracy: 0.9730 - val_loss: 0.5470 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2605 - accuracy: 0.9730 - val_loss: 0.5493 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2602 - accuracy: 0.9730 - val_loss: 0.5482 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2598 - accuracy: 0.9730 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2595 - accuracy: 0.9730 - val_loss: 0.5474 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2592 - accuracy: 0.9730 - val_loss: 0.5494 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2588 - accuracy: 0.9730 - val_loss: 0.5480 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2584 - accuracy: 0.9730 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2581 - accuracy: 0.9730 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2578 - accuracy: 0.9730 - val_loss: 0.5493 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2575 - accuracy: 0.9730 - val_loss: 0.5482 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2571 - accuracy: 0.9730 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2568 - accuracy: 0.9730 - val_loss: 0.5483 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2564 - accuracy: 0.9730 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2561 - accuracy: 0.9730 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2557 - accuracy: 0.9730 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2554 - accuracy: 0.9730 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2551 - accuracy: 0.9730 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.2547 - accuracy: 0.9730 - val_loss: 0.5495 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.2544 - accuracy: 0.9730 - val_loss: 0.5478 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2541 - accuracy: 0.9730 - val_loss: 0.5499 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2538 - accuracy: 0.9730 - val_loss: 0.5479 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2534 - accuracy: 0.9730 - val_loss: 0.5495 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2531 - accuracy: 0.9730 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2527 - accuracy: 0.9730 - val_loss: 0.5495 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2524 - accuracy: 0.9730 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2521 - accuracy: 0.9730 - val_loss: 0.5500 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2518 - accuracy: 0.9730 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2514 - accuracy: 0.9730 - val_loss: 0.5505 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2511 - accuracy: 0.9730 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2508 - accuracy: 0.9730 - val_loss: 0.5499 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2504 - accuracy: 0.9730 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2501 - accuracy: 0.9730 - val_loss: 0.5500 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2498 - accuracy: 0.9730 - val_loss: 0.5484 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2495 - accuracy: 0.9730 - val_loss: 0.5502 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2491 - accuracy: 0.9730 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2488 - accuracy: 0.9730 - val_loss: 0.5500 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2485 - accuracy: 0.9730 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2482 - accuracy: 0.9730 - val_loss: 0.5496 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2478 - accuracy: 0.9730 - val_loss: 0.5495 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2475 - accuracy: 0.9730 - val_loss: 0.5490 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2472 - accuracy: 0.9730 - val_loss: 0.5502 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2469 - accuracy: 0.9730 - val_loss: 0.5489 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2466 - accuracy: 0.9730 - val_loss: 0.5497 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2462 - accuracy: 0.9730 - val_loss: 0.5494 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2459 - accuracy: 0.9730 - val_loss: 0.5498 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2456 - accuracy: 0.9730 - val_loss: 0.5486 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2453 - accuracy: 0.9730 - val_loss: 0.5500 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2449 - accuracy: 0.9730 - val_loss: 0.5488 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2446 - accuracy: 0.9730 - val_loss: 0.5497 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2443 - accuracy: 0.9730 - val_loss: 0.5493 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2440 - accuracy: 0.9730 - val_loss: 0.5496 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2437 - accuracy: 0.9730 - val_loss: 0.5500 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2433 - accuracy: 0.9730 - val_loss: 0.5492 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2430 - accuracy: 0.9730 - val_loss: 0.5504 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.2427 - accuracy: 0.9730 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2424 - accuracy: 0.9730 - val_loss: 0.5515 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2421 - accuracy: 0.9730 - val_loss: 0.5481 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2418 - accuracy: 0.9730 - val_loss: 0.5513 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2415 - accuracy: 0.9730 - val_loss: 0.5480 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2411 - accuracy: 0.9730 - val_loss: 0.5511 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2408 - accuracy: 0.9730 - val_loss: 0.5489 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2405 - accuracy: 0.9730 - val_loss: 0.5509 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2402 - accuracy: 0.9730 - val_loss: 0.5490 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2399 - accuracy: 0.9730 - val_loss: 0.5504 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2395 - accuracy: 0.9730 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2392 - accuracy: 0.9730 - val_loss: 0.5508 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2389 - accuracy: 0.9730 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2386 - accuracy: 0.9730 - val_loss: 0.5505 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2383 - accuracy: 0.9730 - val_loss: 0.5492 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2380 - accuracy: 0.9730 - val_loss: 0.5508 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2377 - accuracy: 0.9730 - val_loss: 0.5496 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2373 - accuracy: 0.9730 - val_loss: 0.5503 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2371 - accuracy: 0.9730 - val_loss: 0.5496 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2368 - accuracy: 0.9730 - val_loss: 0.5515 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2365 - accuracy: 0.9730 - val_loss: 0.5492 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2361 - accuracy: 0.9730 - val_loss: 0.5516 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2358 - accuracy: 0.9730 - val_loss: 0.5485 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2355 - accuracy: 0.9730 - val_loss: 0.5516 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2352 - accuracy: 0.9730 - val_loss: 0.5492 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2349 - accuracy: 0.9730 - val_loss: 0.5513 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2346 - accuracy: 0.9730 - val_loss: 0.5493 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2343 - accuracy: 0.9730 - val_loss: 0.5518 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2339 - accuracy: 0.9730 - val_loss: 0.5490 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2336 - accuracy: 0.9730 - val_loss: 0.5513 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2333 - accuracy: 0.9730 - val_loss: 0.5497 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2330 - accuracy: 0.9730 - val_loss: 0.5512 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2327 - accuracy: 0.9730 - val_loss: 0.5497 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2324 - accuracy: 0.9730 - val_loss: 0.5511 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2321 - accuracy: 0.9730 - val_loss: 0.5487 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2317 - accuracy: 0.9730 - val_loss: 0.5517 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2314 - accuracy: 0.9730 - val_loss: 0.5495 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2311 - accuracy: 0.9730 - val_loss: 0.5521 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2308 - accuracy: 0.9730 - val_loss: 0.5491 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2305 - accuracy: 0.9730 - val_loss: 0.5521 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2303 - accuracy: 0.9730 - val_loss: 0.5490 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2299 - accuracy: 0.9730 - val_loss: 0.5522 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2296 - accuracy: 0.9730 - val_loss: 0.5496 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2293 - accuracy: 0.9730 - val_loss: 0.5519 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2290 - accuracy: 0.9730 - val_loss: 0.5496 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2286 - accuracy: 0.9730 - val_loss: 0.5521 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2284 - accuracy: 0.9730 - val_loss: 0.5497 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2280 - accuracy: 0.9730 - val_loss: 0.5520 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2277 - accuracy: 0.9730 - val_loss: 0.5506 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2275 - accuracy: 0.9730 - val_loss: 0.5524 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2271 - accuracy: 1.0000 - val_loss: 0.5501 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2268 - accuracy: 0.9730 - val_loss: 0.5523 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2265 - accuracy: 1.0000 - val_loss: 0.5508 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2262 - accuracy: 0.9730 - val_loss: 0.5521 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2259 - accuracy: 1.0000 - val_loss: 0.5510 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2256 - accuracy: 1.0000 - val_loss: 0.5521 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2253 - accuracy: 1.0000 - val_loss: 0.5513 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.2250 - accuracy: 1.0000 - val_loss: 0.5520 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2247 - accuracy: 1.0000 - val_loss: 0.5520 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2244 - accuracy: 1.0000 - val_loss: 0.5523 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2241 - accuracy: 1.0000 - val_loss: 0.5510 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2238 - accuracy: 1.0000 - val_loss: 0.5524 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2235 - accuracy: 1.0000 - val_loss: 0.5519 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2232 - accuracy: 1.0000 - val_loss: 0.5526 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2229 - accuracy: 1.0000 - val_loss: 0.5517 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2226 - accuracy: 1.0000 - val_loss: 0.5533 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2223 - accuracy: 1.0000 - val_loss: 0.5514 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2220 - accuracy: 1.0000 - val_loss: 0.5535 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2217 - accuracy: 1.0000 - val_loss: 0.5518 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2214 - accuracy: 1.0000 - val_loss: 0.5533 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2211 - accuracy: 1.0000 - val_loss: 0.5517 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2208 - accuracy: 1.0000 - val_loss: 0.5532 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2206 - accuracy: 1.0000 - val_loss: 0.5518 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2202 - accuracy: 1.0000 - val_loss: 0.5542 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2200 - accuracy: 1.0000 - val_loss: 0.5513 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2197 - accuracy: 1.0000 - val_loss: 0.5549 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2194 - accuracy: 1.0000 - val_loss: 0.5510 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2191 - accuracy: 1.0000 - val_loss: 0.5543 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2188 - accuracy: 1.0000 - val_loss: 0.5521 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2185 - accuracy: 1.0000 - val_loss: 0.5541 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2182 - accuracy: 1.0000 - val_loss: 0.5520 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2179 - accuracy: 1.0000 - val_loss: 0.5541 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2176 - accuracy: 1.0000 - val_loss: 0.5527 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2173 - accuracy: 1.0000 - val_loss: 0.5542 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2170 - accuracy: 1.0000 - val_loss: 0.5534 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2167 - accuracy: 1.0000 - val_loss: 0.5545 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2165 - accuracy: 1.0000 - val_loss: 0.5533 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2161 - accuracy: 1.0000 - val_loss: 0.5549 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2159 - accuracy: 1.0000 - val_loss: 0.5527 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2156 - accuracy: 1.0000 - val_loss: 0.5549 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2153 - accuracy: 1.0000 - val_loss: 0.5532 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2150 - accuracy: 1.0000 - val_loss: 0.5550 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2147 - accuracy: 1.0000 - val_loss: 0.5540 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2144 - accuracy: 1.0000 - val_loss: 0.5543 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2141 - accuracy: 1.0000 - val_loss: 0.5541 - val_accuracy: 0.6471\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6e19ec35b0>"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:38:39.093587Z",
     "start_time": "2019-10-02T17:31:24.683740Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "# summarize history for loss\n",
    "plt.plot(classifier.history.history['loss'])\n",
    "plt.plot(classifier.history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-18T12:44:00.380262</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 385.78125 277.314375 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me2649406c4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#me2649406c4\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"101.272159\" xlink:href=\"#me2649406c4\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(91.728409 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"143.544886\" xlink:href=\"#me2649406c4\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(134.001136 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.817614\" xlink:href=\"#me2649406c4\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(176.273864 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"228.090341\" xlink:href=\"#me2649406c4\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(218.546591 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"270.363068\" xlink:href=\"#me2649406c4\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(260.819318 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"312.635795\" xlink:href=\"#me2649406c4\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(303.092045 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"354.908523\" xlink:href=\"#me2649406c4\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(345.364773 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- epoch -->\n     <g transform=\"translate(195.953125 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-68\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"ma2e8f5189e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma2e8f5189e\" y=\"233.758768\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 237.557987)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma2e8f5189e\" y=\"206.241811\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.3 -->\n      <g transform=\"translate(20.878125 210.041029)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma2e8f5189e\" y=\"178.724853\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 182.524072)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma2e8f5189e\" y=\"151.207896\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.5 -->\n      <g transform=\"translate(20.878125 155.007115)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma2e8f5189e\" y=\"123.690939\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 127.490157)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma2e8f5189e\" y=\"96.173981\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.7 -->\n      <g transform=\"translate(20.878125 99.9732)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-37\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma2e8f5189e\" y=\"68.657024\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 72.456243)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma2e8f5189e\" y=\"41.140067\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.9 -->\n      <g transform=\"translate(20.878125 44.939285)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 140.695937)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p6a3852517d)\" d=\"M 58.999432 63.491552 \nL 59.422159 83.034392 \nL 59.844886 93.771707 \nL 60.267614 99.15282 \nL 60.690341 102.072874 \nL 61.535795 105.521412 \nL 62.38125 107.953393 \nL 64.072159 111.883034 \nL 65.763068 115.209596 \nL 67.876705 118.888065 \nL 70.835795 123.587193 \nL 73.794886 127.542633 \nL 78.444886 133.029028 \nL 82.249432 137.032507 \nL 84.785795 139.375412 \nL 91.972159 145.426965 \nL 95.776705 148.227732 \nL 105.076705 154.151338 \nL 111.840341 157.909863 \nL 116.913068 160.45767 \nL 121.563068 162.65539 \nL 126.635795 164.983484 \nL 139.317614 170.314541 \nL 162.990341 179.127112 \nL 169.753977 181.44717 \nL 172.290341 182.331327 \nL 174.403977 183.089342 \nL 182.013068 185.772305 \nL 187.085795 187.425245 \nL 190.890341 188.616781 \nL 214.140341 195.695897 \nL 216.676705 196.400025 \nL 218.790341 197.006524 \nL 241.617614 203.073872 \nL 247.535795 204.601972 \nL 262.753977 208.373487 \nL 291.922159 215.164759 \nL 304.603977 217.945778 \nL 313.48125 219.884528 \nL 327.43125 222.786114 \nL 334.194886 224.161944 \nL 340.535795 225.431513 \nL 343.917614 226.129536 \nL 349.835795 227.296675 \nL 361.672159 229.552406 \nL 363.363068 229.874489 \nL 363.363068 229.874489 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p6a3852517d)\" d=\"M 58.999432 32.201761 \nL 59.422159 59.539852 \nL 59.844886 75.133075 \nL 60.267614 84.342206 \nL 60.690341 90.418247 \nL 61.113068 94.533157 \nL 61.958523 99.651997 \nL 62.803977 102.682562 \nL 63.649432 104.843822 \nL 64.494886 106.540513 \nL 65.763068 108.467298 \nL 67.03125 110.106748 \nL 69.144886 112.390066 \nL 69.990341 113.146613 \nL 70.413068 113.352614 \nL 70.835795 113.817544 \nL 71.258523 114.03465 \nL 72.103977 114.688097 \nL 72.526705 114.848076 \nL 73.372159 115.577019 \nL 74.640341 116.444374 \nL 76.33125 117.474955 \nL 76.753977 117.619697 \nL 77.176705 117.952826 \nL 77.599432 118.109393 \nL 78.022159 118.460891 \nL 78.867614 118.776848 \nL 79.290341 119.105581 \nL 79.713068 119.211697 \nL 80.135795 119.463672 \nL 80.98125 119.770001 \nL 81.826705 120.105376 \nL 82.249432 120.493006 \nL 83.094886 120.925035 \nL 83.940341 121.372432 \nL 86.476705 122.86076 \nL 86.899432 122.962596 \nL 87.322159 123.309403 \nL 87.744886 123.380962 \nL 88.167614 123.723046 \nL 89.435795 124.116663 \nL 89.858523 124.349349 \nL 90.28125 124.437556 \nL 91.549432 124.933452 \nL 93.240341 125.489869 \nL 93.663068 125.720751 \nL 94.93125 126.003593 \nL 95.776705 126.31558 \nL 96.622159 126.525911 \nL 100.426705 127.659198 \nL 102.117614 128.103971 \nL 102.540341 128.140956 \nL 102.963068 128.46736 \nL 103.385795 128.422436 \nL 104.23125 128.76001 \nL 104.653977 128.798274 \nL 105.076705 129.061467 \nL 105.922159 129.205668 \nL 106.767614 129.421002 \nL 116.490341 130.654944 \nL 116.913068 130.794405 \nL 117.758523 130.78212 \nL 118.18125 131.018808 \nL 118.603977 130.938917 \nL 119.026705 131.116232 \nL 119.872159 131.165273 \nL 120.717614 131.383493 \nL 121.563068 131.435682 \nL 123.253977 131.923673 \nL 123.676705 131.749506 \nL 124.099432 131.891657 \nL 124.522159 131.885556 \nL 124.944886 132.067562 \nL 125.367614 131.972106 \nL 125.790341 132.118505 \nL 126.213068 131.901252 \nL 126.635795 132.268987 \nL 127.058523 132.097101 \nL 127.48125 132.266527 \nL 127.903977 132.267036 \nL 128.326705 132.478171 \nL 128.749432 132.382157 \nL 129.172159 132.606561 \nL 129.594886 132.472611 \nL 130.017614 132.658077 \nL 130.440341 132.611809 \nL 131.285795 132.795652 \nL 131.708523 132.875084 \nL 132.13125 132.811086 \nL 132.553977 133.025911 \nL 136.358523 133.604027 \nL 136.78125 133.492317 \nL 137.203977 133.781622 \nL 137.626705 133.664516 \nL 138.049432 133.919984 \nL 138.472159 133.757741 \nL 139.740341 134.021672 \nL 140.163068 133.802894 \nL 140.585795 134.033809 \nL 141.43125 134.2003 \nL 141.853977 134.020409 \nL 142.276705 134.062282 \nL 142.699432 134.246929 \nL 143.544886 134.150095 \nL 143.967614 134.282503 \nL 144.390341 134.264314 \nL 145.658523 134.454849 \nL 147.349432 134.612466 \nL 147.772159 134.463247 \nL 148.194886 134.707545 \nL 149.463068 134.69326 \nL 149.885795 134.8721 \nL 150.73125 134.916647 \nL 151.153977 135.013612 \nL 151.576705 134.926848 \nL 152.844886 135.157698 \nL 154.535795 135.172557 \nL 155.38125 135.365388 \nL 155.803977 135.23234 \nL 156.226705 135.5079 \nL 156.649432 135.245232 \nL 157.072159 135.66888 \nL 157.494886 135.35481 \nL 157.917614 135.569126 \nL 158.340341 135.403259 \nL 158.763068 135.637176 \nL 159.185795 135.382528 \nL 159.608523 135.669076 \nL 160.03125 135.533486 \nL 160.876705 135.736716 \nL 161.722159 135.714738 \nL 163.835795 135.826579 \nL 164.258523 135.755151 \nL 164.68125 135.945391 \nL 165.103977 135.64946 \nL 165.526705 135.959069 \nL 165.949432 135.75894 \nL 166.372159 135.934894 \nL 166.794886 135.81459 \nL 167.217614 136.094299 \nL 167.640341 135.889331 \nL 168.063068 136.024708 \nL 168.485795 135.915638 \nL 168.908523 136.097628 \nL 169.33125 135.975717 \nL 169.753977 135.971485 \nL 170.176705 136.103861 \nL 170.599432 135.97616 \nL 171.022159 136.121033 \nL 171.444886 135.998137 \nL 172.290341 136.049999 \nL 172.713068 135.851329 \nL 173.135795 136.047555 \nL 173.558523 136.113685 \nL 173.98125 135.867041 \nL 174.403977 136.074256 \nL 174.826705 135.981818 \nL 175.672159 136.254721 \nL 176.517614 136.199612 \nL 177.785795 136.421539 \nL 178.208523 136.290066 \nL 178.63125 136.508122 \nL 179.053977 136.382438 \nL 179.899432 136.528394 \nL 180.322159 136.227576 \nL 180.744886 136.620504 \nL 181.167614 136.288311 \nL 181.590341 136.526328 \nL 182.013068 136.435464 \nL 182.858523 136.566544 \nL 183.28125 136.420785 \nL 184.549432 136.583765 \nL 184.972159 136.42541 \nL 185.817614 136.631067 \nL 186.240341 136.53461 \nL 187.085795 136.683879 \nL 187.93125 136.668314 \nL 189.199432 136.777515 \nL 190.044886 136.766936 \nL 191.313068 136.765263 \nL 191.735795 136.972544 \nL 192.158523 136.68793 \nL 192.58125 136.951255 \nL 193.003977 136.769035 \nL 193.426705 136.9938 \nL 193.849432 136.715714 \nL 194.272159 137.159848 \nL 194.694886 136.71824 \nL 195.117614 137.008643 \nL 195.540341 136.882434 \nL 195.963068 136.977448 \nL 196.385795 136.907627 \nL 196.808523 137.10469 \nL 197.23125 136.876727 \nL 197.653977 137.056273 \nL 200.190341 137.147054 \nL 200.613068 137.015384 \nL 201.88125 137.243987 \nL 202.303977 137.047777 \nL 203.149432 137.238705 \nL 203.572159 137.049138 \nL 203.994886 137.247956 \nL 204.417614 137.313414 \nL 204.840341 137.066556 \nL 205.263068 137.18837 \nL 206.108523 137.114744 \nL 206.53125 137.387236 \nL 207.799432 137.226093 \nL 208.222159 137.410264 \nL 208.644886 137.293158 \nL 209.067614 137.37692 \nL 209.490341 137.320499 \nL 209.913068 137.493895 \nL 210.335795 137.310002 \nL 210.758523 137.511346 \nL 211.18125 137.544165 \nL 211.603977 137.405803 \nL 212.449432 137.490893 \nL 212.872159 137.41876 \nL 213.294886 137.603325 \nL 213.717614 137.391665 \nL 214.140341 137.683609 \nL 214.563068 137.522039 \nL 214.985795 137.61456 \nL 215.408523 137.407754 \nL 215.83125 137.723153 \nL 216.253977 137.428076 \nL 216.676705 137.788431 \nL 217.944886 137.611017 \nL 218.367614 137.782526 \nL 218.790341 137.596813 \nL 219.213068 137.875784 \nL 219.635795 137.648412 \nL 220.058523 137.834896 \nL 224.285795 137.82348 \nL 224.708523 137.861384 \nL 225.13125 138.015672 \nL 225.553977 137.954855 \nL 225.976705 137.776408 \nL 226.399432 137.962466 \nL 226.822159 137.656137 \nL 227.244886 138.045654 \nL 227.667614 137.808932 \nL 228.090341 138.115556 \nL 228.513068 137.86719 \nL 228.935795 137.998254 \nL 229.358523 137.882886 \nL 229.78125 138.097794 \nL 230.203977 137.898878 \nL 230.626705 138.081704 \nL 231.049432 137.919264 \nL 231.472159 138.12789 \nL 231.894886 137.90839 \nL 236.122159 138.124774 \nL 236.544886 137.893481 \nL 236.967614 138.192462 \nL 237.390341 137.875834 \nL 237.813068 138.088756 \nL 238.658523 138.062695 \nL 239.08125 138.011309 \nL 239.503977 138.102862 \nL 239.926705 137.927334 \nL 240.349432 138.205387 \nL 240.772159 137.927383 \nL 241.194886 138.12771 \nL 241.617614 137.959513 \nL 242.040341 138.323477 \nL 242.463068 137.855463 \nL 242.885795 138.248358 \nL 243.308523 137.85379 \nL 243.73125 138.360363 \nL 244.153977 137.952166 \nL 244.576705 138.401252 \nL 244.999432 137.965024 \nL 245.422159 138.369925 \nL 245.844886 137.920806 \nL 246.267614 138.292675 \nL 246.690341 137.83857 \nL 247.113068 138.282637 \nL 247.535795 137.961088 \nL 247.958523 138.266843 \nL 248.38125 138.032073 \nL 248.803977 138.358165 \nL 249.226705 137.838717 \nL 249.649432 138.320442 \nL 250.072159 137.95315 \nL 250.494886 138.534218 \nL 250.917614 137.966878 \nL 251.340341 138.39579 \nL 251.763068 137.940553 \nL 252.185795 138.442173 \nL 252.608523 138.040848 \nL 253.03125 138.558066 \nL 253.453977 137.88141 \nL 253.876705 138.30801 \nL 254.299432 138.163235 \nL 254.722159 138.441403 \nL 255.144886 138.099778 \nL 255.567614 138.495593 \nL 255.990341 138.122002 \nL 256.413068 138.419802 \nL 256.835795 138.073011 \nL 257.258523 138.505302 \nL 257.68125 138.27237 \nL 258.103977 138.313947 \nL 258.526705 138.13678 \nL 259.794886 138.324608 \nL 260.640341 138.249474 \nL 261.063068 138.306124 \nL 261.485795 138.146046 \nL 261.908523 138.256608 \nL 264.444886 138.176766 \nL 264.867614 138.296332 \nL 265.290341 138.067123 \nL 265.713068 138.22474 \nL 266.558523 138.171846 \nL 266.98125 138.097334 \nL 267.403977 138.303565 \nL 267.826705 138.055084 \nL 268.249432 138.320606 \nL 269.094886 138.057364 \nL 269.517614 138.331546 \nL 269.940341 138.062793 \nL 270.363068 138.312422 \nL 271.63125 138.16686 \nL 273.744886 138.259823 \nL 274.167614 138.032975 \nL 274.590341 138.256018 \nL 275.013068 137.943522 \nL 275.435795 138.222411 \nL 275.858523 137.848033 \nL 276.28125 138.367547 \nL 276.703977 137.882411 \nL 277.126705 138.219016 \nL 277.549432 138.014064 \nL 277.972159 138.385195 \nL 278.394886 137.806489 \nL 278.817614 138.566545 \nL 279.240341 137.74541 \nL 279.663068 138.313176 \nL 280.085795 137.964368 \nL 280.508523 138.418539 \nL 280.93125 137.831304 \nL 281.353977 138.386081 \nL 281.776705 137.881328 \nL 282.199432 138.469629 \nL 282.622159 137.798632 \nL 283.044886 138.332727 \nL 283.467614 137.937798 \nL 283.890341 138.038716 \nL 284.313068 138.322509 \nL 284.735795 137.743556 \nL 285.158523 138.365727 \nL 285.58125 137.856923 \nL 286.003977 138.161743 \nL 286.426705 137.575672 \nL 286.849432 138.363562 \nL 287.272159 137.80065 \nL 287.694886 138.230087 \nL 288.117614 137.860482 \nL 288.540341 138.088035 \nL 288.963068 137.866583 \nL 289.385795 138.139141 \nL 289.808523 137.931369 \nL 290.23125 137.920265 \nL 290.653977 138.118049 \nL 291.076705 137.801945 \nL 291.499432 138.090036 \nL 291.922159 137.81008 \nL 292.344886 138.004355 \nL 292.767614 137.906471 \nL 293.190341 138.082196 \nL 293.613068 137.747001 \nL 294.035795 137.987904 \nL 294.88125 137.781394 \nL 295.303977 138.011096 \nL 295.726705 137.691384 \nL 296.149432 138.185656 \nL 296.572159 137.686086 \nL 296.994886 138.331694 \nL 297.417614 137.707441 \nL 297.840341 138.120641 \nL 298.263068 137.68256 \nL 298.685795 138.020035 \nL 299.108523 137.571391 \nL 299.53125 138.145948 \nL 299.953977 137.590121 \nL 300.376705 138.264546 \nL 300.799432 137.63001 \nL 301.222159 137.944227 \nL 301.644886 137.841423 \nL 302.067614 138.175356 \nL 302.490341 137.617971 \nL 302.913068 138.011883 \nL 303.335795 137.693434 \nL 303.758523 137.962318 \nL 304.18125 137.65248 \nL 304.603977 137.944818 \nL 305.026705 137.83688 \nL 305.449432 137.919428 \nL 306.294886 137.804028 \nL 306.717614 137.863483 \nL 307.140341 137.703816 \nL 307.563068 137.875571 \nL 307.985795 137.576541 \nL 308.408523 138.045982 \nL 308.83125 137.478887 \nL 309.253977 138.038125 \nL 309.676705 137.578148 \nL 310.099432 137.872373 \nL 310.522159 137.580067 \nL 310.944886 137.8089 \nL 311.367614 137.449004 \nL 311.790341 137.960448 \nL 312.213068 137.32314 \nL 312.635795 137.860761 \nL 313.058523 137.465225 \nL 313.48125 137.806833 \nL 313.903977 137.456827 \nL 314.326705 137.89709 \nL 314.749432 137.381463 \nL 315.172159 137.70211 \nL 315.594886 137.447003 \nL 316.017614 137.824005 \nL 316.440341 137.553415 \nL 316.863068 137.584496 \nL 317.285795 137.730813 \nL 317.708523 137.400423 \nL 318.13125 137.740735 \nL 318.553977 137.53475 \nL 318.976705 137.605014 \nL 319.399432 137.500275 \nL 319.822159 137.837258 \nL 320.244886 137.451727 \nL 320.667614 137.773915 \nL 321.090341 137.535915 \nL 321.513068 137.649035 \nL 322.358523 137.43913 \nL 322.78125 137.679903 \nL 323.203977 137.342379 \nL 323.626705 137.982475 \nL 324.049432 137.049155 \nL 324.472159 137.981376 \nL 324.894886 137.087698 \nL 325.317614 137.988019 \nL 325.740341 137.160241 \nL 326.163068 137.764321 \nL 326.585795 137.213382 \nL 327.008523 137.738193 \nL 327.43125 137.336835 \nL 327.853977 137.708425 \nL 328.276705 137.226454 \nL 328.699432 137.710754 \nL 329.122159 137.30418 \nL 329.544886 137.657334 \nL 329.967614 137.22068 \nL 330.390341 137.550873 \nL 330.813068 137.363307 \nL 331.235795 137.563453 \nL 331.658523 137.028161 \nL 332.08125 137.663485 \nL 332.503977 137.013039 \nL 332.926705 137.869207 \nL 333.349432 137.014138 \nL 333.772159 137.666404 \nL 334.194886 137.093176 \nL 334.617614 137.646624 \nL 335.040341 136.944514 \nL 335.463068 137.712705 \nL 335.885795 137.082712 \nL 336.308523 137.523876 \nL 336.73125 137.127488 \nL 337.153977 137.527862 \nL 337.576705 137.154861 \nL 337.999432 137.805603 \nL 338.422159 136.992176 \nL 338.844886 137.590007 \nL 339.267614 136.874791 \nL 339.690341 137.69286 \nL 340.113068 136.884255 \nL 340.535795 137.72499 \nL 340.958523 136.838643 \nL 341.38125 137.554088 \nL 341.803977 136.927128 \nL 342.226705 137.554694 \nL 342.649432 136.867903 \nL 343.072159 137.525828 \nL 343.494886 136.901477 \nL 343.917614 137.289369 \nL 344.340341 136.798361 \nL 344.763068 137.419153 \nL 345.185795 136.814008 \nL 345.608523 137.237803 \nL 346.03125 136.859391 \nL 346.453977 137.179349 \nL 346.876705 136.86869 \nL 347.299432 137.07994 \nL 347.722159 136.886305 \nL 348.567614 136.816353 \nL 348.990341 137.171164 \nL 349.413068 136.784092 \nL 349.835795 136.935411 \nL 350.258523 136.72836 \nL 350.68125 136.983254 \nL 351.103977 136.540712 \nL 351.526705 137.069591 \nL 351.949432 136.492328 \nL 352.372159 136.945481 \nL 352.794886 136.549913 \nL 353.217614 136.974955 \nL 353.640341 136.570661 \nL 354.063068 136.954732 \nL 354.485795 136.296019 \nL 354.908523 137.08363 \nL 355.33125 136.097349 \nL 355.753977 137.18358 \nL 356.176705 136.255344 \nL 356.599432 136.860752 \nL 357.022159 136.309108 \nL 357.444886 136.889011 \nL 357.867614 136.325017 \nL 358.290341 136.693589 \nL 358.713068 136.304023 \nL 359.135795 136.520653 \nL 359.558523 136.217588 \nL 359.98125 136.528657 \nL 360.403977 136.097661 \nL 360.826705 136.711844 \nL 361.249432 136.102663 \nL 361.672159 136.581256 \nL 362.094886 136.070976 \nL 362.517614 136.356901 \nL 362.940341 136.26156 \nL 363.363068 136.310912 \nL 363.363068 136.310912 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_19\">\n    <!-- model loss -->\n    <g transform=\"translate(179.001563 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-6d\"/>\n     <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n     <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     <use x=\"311.376953\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"343.164062\" xlink:href=\"#DejaVuSans-6c\"/>\n     <use x=\"370.947266\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"432.128906\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"484.228516\" xlink:href=\"#DejaVuSans-73\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 50.78125 59.674375 \nL 106.05625 59.674375 \nQ 108.05625 59.674375 108.05625 57.674375 \nL 108.05625 29.318125 \nQ 108.05625 27.318125 106.05625 27.318125 \nL 50.78125 27.318125 \nQ 48.78125 27.318125 48.78125 29.318125 \nL 48.78125 57.674375 \nQ 48.78125 59.674375 50.78125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 52.78125 35.416562 \nL 72.78125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_20\">\n     <!-- train -->\n     <g transform=\"translate(80.78125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-6e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 52.78125 50.094687 \nL 72.78125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_22\"/>\n    <g id=\"text_21\">\n     <!-- test -->\n     <g transform=\"translate(80.78125 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6a3852517d\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:13:22.174186Z",
     "start_time": "2019-10-02T17:13:21.959174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "classifier.save('./models/bestmodel')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-18 12:45:44.473958: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/bestmodel/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "#model = load_model('./Saved Models/model360-2509-653.h5')\n",
    "# model = load_model('bestmodel.h5')\n",
    "# model.summary()\n",
    "model = classifier"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:40:30.689969Z",
     "start_time": "2019-10-02T17:40:29.473900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "#Accuracy of test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred > 0.5)\n",
    "print(boldTextStart,'Accuracy:',boldTextEnd)\n",
    "accuracy_model = accuracy_score(y_test, y_pred, normalize=True)*100\n",
    "print(accuracy_model,' %')\n",
    "print(boldTextStart,'Confusion Matrix:',boldTextEnd)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(boldTextStart,'Classification Report:',boldTextEnd)\n",
    "print(classification_report(y_test,y_pred))\n",
    "auc = roc_auc_score(y_test,y_pred)\n",
    "print(boldTextStart,'AUC Score: ',boldTextEnd, auc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m Accuracy: \u001b[0;0m\n",
      "50.0  %\n",
      "\u001b[1m Confusion Matrix: \u001b[0;0m\n",
      "[[5 3]\n",
      " [4 2]]\n",
      "\u001b[1m Classification Report: \u001b[0;0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.62      0.59         8\n",
      "           1       0.40      0.33      0.36         6\n",
      "\n",
      "    accuracy                           0.50        14\n",
      "   macro avg       0.48      0.48      0.48        14\n",
      "weighted avg       0.49      0.50      0.49        14\n",
      "\n",
      "\u001b[1m AUC Score:  \u001b[0;0m 0.47916666666666663\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T17:40:31.264002Z",
     "start_time": "2019-10-02T17:40:31.157996Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}